{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from vocab import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23f30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccddc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, tokens_list):\n",
    "        super().__init__()\n",
    "        self.tokens_list    = tokens_list\n",
    "        self.max_tokens     = 10\n",
    "        self.context_length = 64\n",
    "        self.vocab_size     = 200\n",
    "        self.n_layers       = 2      # agora duas camadas\n",
    "        self.n_heads        = 4      # agora quatro cabeças\n",
    "        self.n_embd         = 16\n",
    "\n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embd)\n",
    "        self.wpe = nn.Embedding(self.context_length, self.n_embd)\n",
    "\n",
    "        # MLP\n",
    "        self.fc1 = nn.Linear(self.n_embd, 2 * self.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        # Normalização\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "\n",
    "        # Atenção\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.head_dim = self.n_embd // self.n_heads\n",
    "\n",
    "        # Máscara causal\n",
    "        self.mask = torch.tril(torch.ones(self.context_length,\n",
    "                                          self.context_length)\n",
    "                               ).view(self.context_length,\n",
    "                                      self.context_length)\n",
    "\n",
    "        # Final head\n",
    "        self.final_ln = nn.LayerNorm(self.n_embd)\n",
    "        self.lm_head   = nn.Linear(self.n_embd, self.vocab_size)\n",
    "\n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "    def mlp(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def layer_norm(self, x):\n",
    "        return self.ln(x)\n",
    "\n",
    "    def self_attention(self, x):\n",
    "        # x: (T, C)\n",
    "        T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)               # (T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)        # cada um (T, C)\n",
    "\n",
    "        # projeção multi-head\n",
    "        q = q.view(T, self.n_heads, self.head_dim).transpose(0,1)  # (nh, T, hd)\n",
    "        k = k.view(T, self.n_heads, self.head_dim).transpose(0,1)\n",
    "        v = v.view(T, self.n_heads, self.head_dim).transpose(0,1)\n",
    "\n",
    "        # produto escalar e escala\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim)   # (nh, T, T)\n",
    "\n",
    "        # máscara causal\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # softmax\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # aplicação sobre v\n",
    "        y = att @ v  # (nh, T, hd)\n",
    "\n",
    "        # concatena heads\n",
    "        y = y.transpose(0,1).contiguous().view(T, C)  # (T, C)\n",
    "\n",
    "        # projeção final\n",
    "        return self.out_proj(y)\n",
    "\n",
    "    def tokens_idx(self, tokens_chosen):\n",
    "        self.tokens_vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.tokens_vocab.items()}\n",
    "\n",
    "        # Pegar os índices correspondentes\n",
    "        indices = [self.tokens_vocab[token] for token in tokens_chosen]\n",
    "\n",
    "        # Converter para tensor, se quiser passar ao modelo\n",
    "        self.indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def forward(self):\n",
    "        # 1) lookup embeddings\n",
    "        idx = self.indices_tensor         # (T,)\n",
    "        T   = idx.size(0)\n",
    "        tok_emb = self.wte(idx)           # (T, n_embd)\n",
    "        pos_emb = self.wpe(torch.arange(T, device=idx.device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb             # (T, n_embd)\n",
    "\n",
    "        # 2) aplica N camadas de (LN → Atenção → Residual → LN → MLP → Residual)\n",
    "        for _ in range(self.n_layers):\n",
    "            # pré-atenção\n",
    "            x_norm = self.layer_norm(x)\n",
    "            attn_out = self.self_attention(x_norm)\n",
    "            x = x + attn_out\n",
    "\n",
    "            # pré-MLP\n",
    "            x_norm = self.layer_norm(x)\n",
    "            mlp_out = self.mlp(x_norm)\n",
    "            x = x + mlp_out\n",
    "\n",
    "        return x[-1]  # vetor do último token (n_embd,)\n",
    "\n",
    "    def predict_logits(self, x):\n",
    "        x = self.final_ln(x)  # (n_embd,)\n",
    "        return self.lm_head(x)  # (vocab_size,)\n",
    "\n",
    "    def predict_next_token(self):\n",
    "        self.tokens_idx(self.tokens_list)\n",
    "        vec = self.forward()               # (n_embd,)\n",
    "        logits = self.predict_logits(vec)  # (vocab_size,)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx  = torch.multinomial(probs, num_samples=1).item()\n",
    "        return self.idx_to_token[idx]\n",
    "\n",
    "    def predict_all_sentence(self):\n",
    "        for step in range(self.max_tokens):\n",
    "            nt = self.predict_next_token()\n",
    "            self.tokens_list.append(nt)\n",
    "            print(f\"Passo {step+1}: {' '.join(self.tokens_list)}\")\n",
    "        return self.tokens_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4204617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [\"o\", \"gato\", \"pequeno\"]\n",
    "model = GPT(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "539ba1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (wte): Embedding(200, 16)\n",
      "  (wpe): Embedding(64, 16)\n",
      "  (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (gelu): GELU(approximate='tanh')\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (qkv_proj): Linear(in_features=16, out_features=48, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (final_ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=16, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6833db7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'idx_to_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx_to_token\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\OneDrive\\Desktop\\projetos\\Generative-AI\\gpt\\gpt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'idx_to_token'"
     ]
    }
   ],
   "source": [
    "model.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45d1f33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rio'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5d72df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: o gato pequeno vender\n",
      "Passo 2: o gato pequeno vender e\n",
      "Passo 3: o gato pequeno vender e saber\n",
      "Passo 4: o gato pequeno vender e saber com\n",
      "Passo 5: o gato pequeno vender e saber com odiar\n",
      "Passo 6: o gato pequeno vender e saber com odiar cidade\n",
      "Passo 7: o gato pequeno vender e saber com odiar cidade amiga\n",
      "Passo 8: o gato pequeno vender e saber com odiar cidade amiga conhecer\n",
      "Passo 9: o gato pequeno vender e saber com odiar cidade amiga conhecer umas\n",
      "Passo 10: o gato pequeno vender e saber com odiar cidade amiga conhecer umas estudar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'gato',\n",
       " 'pequeno',\n",
       " 'vender',\n",
       " 'e',\n",
       " 'saber',\n",
       " 'com',\n",
       " 'odiar',\n",
       " 'cidade',\n",
       " 'amiga',\n",
       " 'conhecer',\n",
       " 'umas',\n",
       " 'estudar']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all_sentence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
