{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708f0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from gpt_model import GPT\n",
    "from vocab import tokens\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f3ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocês']\n",
      "podem\n",
      "['vocês', 'podem']\n",
      "explorar\n",
      "['vocês', 'podem', 'explorar']\n",
      "a\n",
      "['vocês', 'podem', 'explorar', 'a']\n",
      "floresta\n",
      "['vocês', 'podem', 'explorar', 'a', 'floresta']\n",
      "durante\n",
      "['vocês', 'podem', 'explorar', 'a', 'floresta', 'durante']\n",
      "o\n",
      "['vocês', 'podem', 'explorar', 'a', 'floresta', 'durante', 'o']\n",
      "dia\n",
      "['ele']\n",
      "vai\n",
      "['ele', 'vai']\n",
      "vender\n",
      "['ele', 'vai', 'vender']\n",
      "um\n",
      "['ele', 'vai', 'vender', 'um']\n",
      "livro\n",
      "['ele', 'vai', 'vender', 'um', 'livro']\n",
      "no\n",
      "['ele', 'vai', 'vender', 'um', 'livro', 'no']\n",
      "mercado\n",
      "['ele', 'vai', 'vender', 'um', 'livro', 'no', 'mercado']\n",
      "amanhã\n",
      "['nós']\n",
      "estamos\n",
      "['nós', 'estamos']\n",
      "felizes\n",
      "['nós', 'estamos', 'felizes']\n",
      "porque\n",
      "['nós', 'estamos', 'felizes', 'porque']\n",
      "tivemos\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos']\n",
      "uma\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma']\n",
      "experiência\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência']\n",
      "interessante\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência', 'interessante']\n",
      "na\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência', 'interessante', 'na']\n",
      "universidade\n"
     ]
    }
   ],
   "source": [
    "# Lista de frases como listas de tokens\n",
    "all_tokens = [\n",
    "    [\"vocês\", \"podem\",   \"explorar\", \"a\", \"floresta\", \"durante\", \"o\", \"dia\"],\n",
    "    [\"ele\",   \"vai\",     \"vender\",   \"um\",\"livro\",    \"no\",      \"mercado\",\"amanhã\"],\n",
    "    [\"nós\",   \"estamos\", \"felizes\",  \"porque\",\"tivemos\",\"uma\",\"experiência\",\"interessante\",\"na\",\"universidade\"]\n",
    "]\n",
    "\n",
    "\n",
    "# Inicializa as listas de inputs e targets\n",
    "inputs, targets = [], []\n",
    "\n",
    "# Para cada frase, acumula pares (contexto → próximo token)\n",
    "for tokens in all_tokens:\n",
    "    for i in range(1, len(tokens)):\n",
    "        # contexto: tokens até a posição i (excluindo i)\n",
    "        inputs.append(tokens[:i])\n",
    "        print(tokens[:i])\n",
    "        # target: token na posição i\n",
    "        targets.append(tokens[i])\n",
    "        print(tokens[i])\n",
    "\n",
    "# Exemplo de saída:\n",
    "# inputs  = [[\"o\"], [\"o\",\"gato\"], [\"o\",\"gato\",\"dorme\"], ..., [\"nós\",\"vai\",\"para\",\"escola\"]]\n",
    "# targets = [\"gato\",\"dorme\",\"em\", ..., \"amanhã\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19bf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b93368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07adcab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6589fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['vocês'], 'podem')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(inputs, targets))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7871905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.tokens_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb4c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_9844\\253647130.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(torch.tensor(logits), target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/5 — Loss: 5.4712\n",
      "Época 2/5 — Loss: 5.6101\n",
      "Época 3/5 — Loss: 5.7878\n",
      "Época 4/5 — Loss: 5.6802\n",
      "Época 5/5 — Loss: 5.3877\n"
     ]
    }
   ],
   "source": [
    "# supondo:\n",
    "# - dataloader retorna pares (tokens_batch, target_batch), onde:\n",
    "#     tokens_batch: lista de tokens (ou tensor de índices) com shape (T,)\n",
    "#     target_batch: índice do token seguinte, tensor escalar ou shape (1,)\n",
    "# - model é uma instância de GPT\n",
    "# - optimizer e criterion foram definidos (e.g. nn.CrossEntropyLoss)\n",
    "num_epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for input_tokens, target_token in zip(inputs, targets):\n",
    "        model = GPT(input_tokens)\n",
    "        # 1) configura os índices de input no modelo\n",
    "        model.tokens_idx(input_tokens)\n",
    "\n",
    "        # 2) forward para obter embedding do último token\n",
    "        last_hidden = model.forward()           # tensor shape (n_embd,)\n",
    "\n",
    "        # 3) projeção final para logits\n",
    "        logits = model.predict_logits(last_hidden)  # shape (vocab_size,)\n",
    "\n",
    "        # 4) compute loss (CrossEntropyLoss espera logits shape (C,) or (1,C) e target single int)\n",
    "        target = torch.tensor(model.tokens_vocab[target_token])\n",
    "        loss = criterion(torch.tensor(logits), target)\n",
    "\n",
    "        # 5) backward + step\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # média da época e registro\n",
    "    avg_loss = epoch_loss / len(inputs)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Época {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0aaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss(torch.tensor([[0.1, 2.9]]), torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8371c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Converte tokens → índices\n",
    "inputs_idx  = [[vocab[t] for t in seq] for seq in inputs]\n",
    "targets_idx = [vocab[t]          for t in targets]\n",
    "\n",
    "\n",
    "X = [pad_truncate(seq, context_length, pad_id) for seq in inputs_idx]\n",
    "Y = targets_idx  # cada Y[i] já é um escalar\n",
    "\n",
    "# 3) Tensores e DataLoader\n",
    "X = torch.tensor(X, dtype=torch.long)   # (N, context_length)\n",
    "Y = torch.tensor(Y, dtype=torch.long)   # (N,)\n",
    "ds = TensorDataset(X, Y)\n",
    "loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 4) Otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "# 5) Scheduler com warmup + linear decay\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(\n",
    "        0.0,\n",
    "        float(total_steps - current_step) / float(max(1, total_steps - warmup_steps))\n",
    "    )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adba3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Loop de treinamento\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        # forward\n",
    "        model.tokens_idx([ idx_to_token[id.item()] for id in batch_x[0] ])  # adapte se necessário\n",
    "        logits = model.forward()        # retorna (n_embd,), se seu forward for assim ajuste\n",
    "        # aqui, se seu forward retorna apenas último token, use batchization:\n",
    "        # logits = model(batch_x)  # se tiver implementado batch no forward\n",
    "        # assume logits: (batch_size, vocab_size) ou (vocab_size,) + reshape\n",
    "        # loss\n",
    "        loss = F.cross_entropy(logits.view(-1, model.vocab_size),\n",
    "                               batch_y.view(-1),\n",
    "                               ignore_index=pad_id)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        #if step % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Step {step}/{len(loader)} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    # opcional: validação aqui\n",
    "\n",
    "# ao final, salve checkpoint\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"epoch\": epoch,\n",
    "    \"scheduler\": scheduler.state_dict(),\n",
    "}, \"checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1523b5db",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca9c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da159ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539f18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
