{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from vocab import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, tokens_list):\n",
    "        super().__init__()\n",
    "        self.tokens_list = tokens_list\n",
    "        self.max_tokens = 10\n",
    "        self.context_length = 64\n",
    "        self.vocab_size = 200\n",
    "        self.n_layers = 1\n",
    "        self.n_heads = 1\n",
    "        self.n_embd = 16\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embd)     # Word Token Embedding\n",
    "        self.wpe = nn.Embedding(self.context_length, self.n_embd) # Positional Embedding\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_embd, 2 * self.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')  # GELU usada no GPT\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3 * self.n_embd)  # Projeção para Q, K, V\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)      # Projeção final\n",
    "        self.head_dim = self.n_embd // self.n_heads              # Dimensão de cada cabeça\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(self.context_length, self.context_length))\n",
    "                .view(1, 1, self.context_length, self.context_length)\n",
    "        )\n",
    "        \n",
    "        self.final_ln = nn.LayerNorm(self.n_embd)  # LayerNorm final\n",
    "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size)  # Mapeia para o vocabulário\n",
    "\n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "    def mlp(self, x):\n",
    "        # x: (T, C)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def layer_norm(self, x):\n",
    "        # x: (T, C)\n",
    "        return self.ln(x)\n",
    "    \n",
    "    def self_attention(self, x):\n",
    "        # x: (T, C) → sequência, embedding\n",
    "        T, C = x.size()\n",
    "\n",
    "        # Projeta para Q, K, V\n",
    "        qkv = self.qkv_proj(x)  # (T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)  # Cada um (T, C)\n",
    "\n",
    "        # Separa em múltiplas heads\n",
    "        #q = q.view(T, self.n_heads, self.head_dim).transpose(0, 1)  # (nh, T, hd)\n",
    "        #k = k.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        #v = v.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # Produto escalar entre Q e Kᵀ\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (nh, T, T)\n",
    "\n",
    "        # Aplicar máscara causal\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Normaliza com Softmax\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # Atenção aplicada sobre V\n",
    "        y = att @ v  # (nh, T, hd)\n",
    "\n",
    "        # Junta as heads\n",
    "        #y = y.transpose(0, 1).contiguous().view(T, C)  # (T, C)\n",
    "\n",
    "        # Projeção final\n",
    "        y = self.out_proj(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def tokens_idx(self, tokens_chosen):\n",
    "        self.tokens_vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.tokens_vocab.items()}\n",
    "\n",
    "        # Pegar os índices correspondentes\n",
    "        indices = [self.tokens_vocab[token] for token in tokens_chosen]\n",
    "\n",
    "        # Converter para tensor, se quiser passar ao modelo\n",
    "        self.indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        #print(\"Tokens escolhidos:\", tokens_escolhidos)\n",
    "        #print(\"Índices encontrados:\", indices)\n",
    "        #print(\"Tensor de índices:\", indices_tensor)\n",
    "        #return indices_tensor\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Executa a passagem para frente (forward pass) do modelo.\n",
    "        Usa os índices de tokens armazenados em self.indices_tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # === Entrada: índices dos tokens ===\n",
    "        idx = self.indices_tensor  # Tensor de índices (T,)\n",
    "        T = idx.size(0)             # Número de tokens\n",
    "        #print(f\"Tamanho da sequência (T): {T}\")\n",
    "        #print(f\"Shape de idx: {idx.shape}\")  # (T,)\n",
    "\n",
    "        # === Embedding dos tokens ===\n",
    "        tok_emb = self.wte(idx)  # Embedding dos tokens (T, n_embd)\n",
    "        #print(\"\\nEmbedding dos Tokens (tok_emb):\")\n",
    "        #print(f\"Shape: {tok_emb.shape}\")  # (T, n_embd)\n",
    "        #print(tok_emb)\n",
    "\n",
    "        # === Embedding das posições ===\n",
    "        positions = torch.arange(T, dtype=torch.long, device=idx.device)  # (T,)\n",
    "        pos_emb = self.wpe(positions)  # Embedding das posições (T, n_embd)\n",
    "        #print(\"\\nEmbedding das Posições (pos_emb):\")\n",
    "        #print(f\"Shape: {pos_emb.shape}\")  # (T, n_embd)\n",
    "        #print(pos_emb)\n",
    "\n",
    "        # === Soma dos embeddings ===\n",
    "        x = tok_emb + pos_emb  # Combinação token + posição (T, n_embd)\n",
    "        #print(\"\\nSoma dos Embeddings (x = tok_emb + pos_emb):\")\n",
    "        #print(f\"Shape: {x.shape}\")  # (T, n_embd)\n",
    "        #print(x)\n",
    "\n",
    "        # === Normalização antes da atenção ===\n",
    "        x_norm = self.layer_norm(x)  # (T, n_embd)\n",
    "        #print(\"\\nLayerNorm aplicado na soma (x_norm):\")\n",
    "        #print(f\"Shape: {x_norm.shape}\")  # (T, n_embd)\n",
    "        #print(x_norm)\n",
    "\n",
    "        # === Atenção (Self-Attention) ===\n",
    "        attn_out = self.self_attention(x_norm)  # (T, n_embd)\n",
    "        #print(\"\\nSaída da Self-Attention (attn_out):\")\n",
    "        #print(f\"Shape: {attn_out.shape}\")  # (T, n_embd)\n",
    "        #print(attn_out)\n",
    "\n",
    "        # === Soma residual (após atenção) ===\n",
    "        x = x + attn_out  # (T, n_embd)\n",
    "        #print(\"\\nSoma Residual após Atenção (x):\")\n",
    "        #print(f\"Shape: {x.shape}\")  # (T, n_embd)\n",
    "        #print(x)\n",
    "\n",
    "        # === Segundo LayerNorm (antes do MLP) ===\n",
    "        x_norm = self.layer_norm(x)  # (T, n_embd)\n",
    "        #print(\"\\nSegundo LayerNorm antes do MLP (x_norm):\")\n",
    "        #print(f\"Shape: {x_norm.shape}\")  # (T, n_embd)\n",
    "        #print(x_norm)\n",
    "\n",
    "        # === Passagem pelo MLP ===\n",
    "        mlp_out = self.mlp(x_norm)  # (T, n_embd)\n",
    "        #print(\"\\nSaída do MLP (mlp_out):\")\n",
    "        #print(f\"Shape: {mlp_out.shape}\")  # (T, n_embd)\n",
    "        #print(mlp_out)\n",
    "\n",
    "        # === Soma Residual após o MLP ===\n",
    "        x = x + mlp_out  # (T, n_embd)\n",
    "        #print(\"\\nSoma Residual após o MLP (x):\")\n",
    "        #print(f\"Shape: {x.shape}\")  # (T, n_embd)\n",
    "        #print(x)\n",
    "\n",
    "        return x[0,0,-1,:]\n",
    "    \n",
    "    def predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        Recebe a saída do forward (T, n_embd),\n",
    "        aplica LayerNorm final e gera logits para o vocabulário (T, vocab_size).\n",
    "        \"\"\"\n",
    "        x = self.final_ln(x)  # Normaliza novamente\n",
    "        logits = self.lm_head(x)  # Projeta para vocab_size\n",
    "        #print(\"\\nLogits (após LayerNorm final):\")\n",
    "        #print(f\"Shape: {logits.shape}\")  # (T, vocab_size)\n",
    "        #print(logits)\n",
    "        return logits\n",
    "\n",
    "    def predict_next_token(self):\n",
    "        self.tokens_idx(self.tokens_list)\n",
    "        res = self.forward()\n",
    "        logits = self.predict_logits(res)\n",
    "        probs = torch.softmax(logits, dim=-1)  # (T, vocab_size)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "        next_token_idx = int(next_token_idx[0])\n",
    "\n",
    "        return self.idx_to_token[next_token_idx]\n",
    "    \n",
    "    def predict_all_sentence(self):\n",
    "        \"\"\"\n",
    "        Gera tokens autoregressivamente até atingir max_tokens,\n",
    "        usando predict_next_token e imprimindo a frase completa a cada passo.\n",
    "        \"\"\"\n",
    "        # Itera até o limite de tokens definidos em self.max_tokens\n",
    "        for step in range(self.max_tokens):\n",
    "            # Prediz o próximo token com base no estado atual\n",
    "            next_token = self.predict_next_token()\n",
    "            # Adiciona o token gerado à lista de tokens\n",
    "            self.tokens_list.append(next_token)\n",
    "            # Constrói a frase atualizada e imprime\n",
    "            sentence = \" \".join(self.tokens_list)\n",
    "            print(f\"Passo {step+1}: {sentence}\")\n",
    "        # Retorna a lista completa de tokens gerados\n",
    "        return self.tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4204617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [\"o\", \"gato\", \"pequeno\"]\n",
    "model = GPT(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45d1f33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dinheiro'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5d72df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: o gato pequeno porta\n",
      "Passo 2: o gato pequeno porta trinta\n",
      "Passo 3: o gato pequeno porta trinta dinheiro\n",
      "Passo 4: o gato pequeno porta trinta dinheiro dez\n",
      "Passo 5: o gato pequeno porta trinta dinheiro dez uns\n",
      "Passo 6: o gato pequeno porta trinta dinheiro dez uns dormir\n",
      "Passo 7: o gato pequeno porta trinta dinheiro dez uns dormir não\n",
      "Passo 8: o gato pequeno porta trinta dinheiro dez uns dormir não escola\n",
      "Passo 9: o gato pequeno porta trinta dinheiro dez uns dormir não escola vocês\n",
      "Passo 10: o gato pequeno porta trinta dinheiro dez uns dormir não escola vocês cinco\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'gato',\n",
       " 'pequeno',\n",
       " 'porta',\n",
       " 'trinta',\n",
       " 'dinheiro',\n",
       " 'dez',\n",
       " 'uns',\n",
       " 'dormir',\n",
       " 'não',\n",
       " 'escola',\n",
       " 'vocês',\n",
       " 'cinco']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all_sentence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
