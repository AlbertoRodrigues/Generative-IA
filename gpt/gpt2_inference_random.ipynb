{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from vocab import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, tokens_list):\n",
    "        super().__init__()\n",
    "        self.tokens_list    = tokens_list\n",
    "        self.max_tokens     = 10\n",
    "        self.context_length = 64\n",
    "        self.vocab_size     = 200\n",
    "        self.n_layers       = 2      # agora duas camadas\n",
    "        self.n_heads        = 4      # agora quatro cabeças\n",
    "        self.n_embd         = 16\n",
    "\n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embd)\n",
    "        self.wpe = nn.Embedding(self.context_length, self.n_embd)\n",
    "\n",
    "        # MLP\n",
    "        self.fc1 = nn.Linear(self.n_embd, 2 * self.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        # Normalização\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "        self.ln.weight.requires_grad = False\n",
    "        self.ln.bias.requires_grad = False\n",
    "\n",
    "        # Atenção\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.head_dim = self.n_embd // self.n_heads\n",
    "\n",
    "        # Máscara causal\n",
    "        self.mask = torch.tril(torch.ones(self.context_length,\n",
    "                                          self.context_length)\n",
    "                               ).view(self.context_length,\n",
    "                                      self.context_length)\n",
    "\n",
    "        # Final head\n",
    "        self.final_ln = nn.LayerNorm(self.n_embd)\n",
    "        self.lm_head   = nn.Linear(self.n_embd, self.vocab_size)\n",
    "\n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "    def mlp(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def layer_norm(self, x):\n",
    "        return self.ln(x)\n",
    "\n",
    "    def self_attention(self, x):\n",
    "        # x: (T, C)\n",
    "        T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)               # (T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)        # cada um (T, C)\n",
    "\n",
    "        # projeção multi-head\n",
    "        q = q.view(T, self.n_heads, self.head_dim).transpose(0,1)  # (nh, T, hd)\n",
    "        k = k.view(T, self.n_heads, self.head_dim).transpose(0,1)\n",
    "        v = v.view(T, self.n_heads, self.head_dim).transpose(0,1)\n",
    "\n",
    "        # produto escalar e escala\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim)   # (nh, T, T)\n",
    "\n",
    "        # máscara causal\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # softmax\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # aplicação sobre v\n",
    "        y = att @ v  # (nh, T, hd)\n",
    "\n",
    "        # concatena heads\n",
    "        y = y.transpose(0,1).contiguous().view(T, C)  # (T, C)\n",
    "\n",
    "        # projeção final\n",
    "        return self.out_proj(y)\n",
    "\n",
    "    def tokens_idx(self, tokens_chosen):\n",
    "        self.tokens_vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.tokens_vocab.items()}\n",
    "\n",
    "        # Pegar os índices correspondentes\n",
    "        indices = [self.tokens_vocab[token] for token in tokens_chosen]\n",
    "\n",
    "        # Converter para tensor, se quiser passar ao modelo\n",
    "        self.indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def token_idx_target(self, token):\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        # 1) lookup embeddings\n",
    "        idx = self.indices_tensor         # (T,)\n",
    "        T   = idx.size(0)\n",
    "        tok_emb = self.wte(idx)           # (T, n_embd)\n",
    "        pos_emb = self.wpe(torch.arange(T, device=idx.device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb             # (T, n_embd)\n",
    "\n",
    "        # 2) aplica N camadas de (LN → Atenção → Residual → LN → MLP → Residual)\n",
    "        for _ in range(self.n_layers):\n",
    "            # pré-atenção\n",
    "            x_norm = self.layer_norm(x)\n",
    "            attn_out = self.self_attention(x_norm)\n",
    "            x = x + attn_out\n",
    "\n",
    "            # pré-MLP\n",
    "            x_norm = self.layer_norm(x)\n",
    "            mlp_out = self.mlp(x_norm)\n",
    "            x = x + mlp_out\n",
    "\n",
    "        return x[-1]  # vetor do último token (n_embd,)\n",
    "\n",
    "    def predict_logits(self, x):\n",
    "        x = self.final_ln(x)  # (n_embd,)\n",
    "        return self.lm_head(x)  # (vocab_size,)\n",
    "\n",
    "    def predict_next_token(self):\n",
    "        self.tokens_idx(self.tokens_list)\n",
    "        vec = self.forward()               # (n_embd,)\n",
    "        logits = self.predict_logits(vec)  # (vocab_size,)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx  = torch.multinomial(probs, num_samples=1).item()\n",
    "        return self.idx_to_token[idx]\n",
    "\n",
    "    def predict_all_sentence(self):\n",
    "        for step in range(self.max_tokens):\n",
    "            nt = self.predict_next_token()\n",
    "            self.tokens_list.append(nt)\n",
    "            print(f\"Passo {step+1}: {' '.join(self.tokens_list)}\")\n",
    "        return self.tokens_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4204617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [\"o\", \"gato\", \"pequeno\"]\n",
    "model = GPT(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "634608e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eu': 0,\n",
       " 'você': 1,\n",
       " 'ele': 2,\n",
       " 'ela': 3,\n",
       " 'nós': 4,\n",
       " 'vocês': 5,\n",
       " 'eles': 6,\n",
       " 'elas': 7,\n",
       " 'me': 8,\n",
       " 'te': 9,\n",
       " 'se': 10,\n",
       " 'nos': 11,\n",
       " 'vos': 12,\n",
       " 'lhe': 13,\n",
       " 'lhes': 14,\n",
       " 'mim': 15,\n",
       " 'ti': 16,\n",
       " 'si': 17,\n",
       " 'ser': 18,\n",
       " 'estar': 19,\n",
       " 'ter': 20,\n",
       " 'haver': 21,\n",
       " 'fazer': 22,\n",
       " 'poder': 23,\n",
       " 'dizer': 24,\n",
       " 'ver': 25,\n",
       " 'dar': 26,\n",
       " 'saber': 27,\n",
       " 'querer': 28,\n",
       " 'chegar': 29,\n",
       " 'passar': 30,\n",
       " 'dever': 31,\n",
       " 'ficar': 32,\n",
       " 'deixar': 33,\n",
       " 'pensar': 34,\n",
       " 'vir': 35,\n",
       " 'conhecer': 36,\n",
       " 'casa': 37,\n",
       " 'tempo': 38,\n",
       " 'dia': 39,\n",
       " 'mundo': 40,\n",
       " 'homem': 41,\n",
       " 'mulher': 42,\n",
       " 'vida': 43,\n",
       " 'mão': 44,\n",
       " 'olho': 45,\n",
       " 'palavra': 46,\n",
       " 'caminho': 47,\n",
       " 'gato': 48,\n",
       " 'cachorro': 49,\n",
       " 'carro': 50,\n",
       " 'livro': 51,\n",
       " 'porta': 52,\n",
       " 'rua': 53,\n",
       " 'trabalho': 154,\n",
       " 'dinheiro': 55,\n",
       " 'noite': 56,\n",
       " 'bom': 57,\n",
       " 'mau': 58,\n",
       " 'feliz': 59,\n",
       " 'triste': 60,\n",
       " 'grande': 61,\n",
       " 'pequeno': 62,\n",
       " 'novo': 63,\n",
       " 'velho': 64,\n",
       " 'forte': 65,\n",
       " 'fraco': 66,\n",
       " 'belo': 67,\n",
       " 'feio': 68,\n",
       " 'inteligente': 69,\n",
       " 'rápido': 70,\n",
       " 'lento': 71,\n",
       " 'em': 72,\n",
       " 'de': 73,\n",
       " 'com': 74,\n",
       " 'por': 75,\n",
       " 'para': 76,\n",
       " 'sobre': 77,\n",
       " 'entre': 78,\n",
       " 'até': 79,\n",
       " 'após': 80,\n",
       " 'antes': 131,\n",
       " 'sem': 82,\n",
       " 'sob': 83,\n",
       " 'contra': 84,\n",
       " 'durante': 85,\n",
       " 'perante': 86,\n",
       " 'trás': 87,\n",
       " 'o': 88,\n",
       " 'a': 89,\n",
       " 'os': 90,\n",
       " 'as': 91,\n",
       " 'um': 104,\n",
       " 'uma': 93,\n",
       " 'uns': 94,\n",
       " 'umas': 95,\n",
       " 'do': 96,\n",
       " 'da': 97,\n",
       " 'dos': 98,\n",
       " 'das': 99,\n",
       " 'este': 100,\n",
       " 'esta': 101,\n",
       " 'estes': 102,\n",
       " 'estas': 103,\n",
       " 'dois': 105,\n",
       " 'três': 106,\n",
       " 'quatro': 107,\n",
       " 'cinco': 108,\n",
       " 'seis': 109,\n",
       " 'sete': 110,\n",
       " 'oito': 111,\n",
       " 'nove': 112,\n",
       " 'dez': 113,\n",
       " 'vinte': 114,\n",
       " 'trinta': 115,\n",
       " 'cem': 116,\n",
       " 'mil': 117,\n",
       " 'milhão': 118,\n",
       " 'sim': 119,\n",
       " 'não': 120,\n",
       " 'talvez': 121,\n",
       " 'hoje': 122,\n",
       " 'amanhã': 123,\n",
       " 'ontem': 124,\n",
       " 'agora': 125,\n",
       " 'sempre': 126,\n",
       " 'nunca': 127,\n",
       " 'já': 128,\n",
       " 'ainda': 129,\n",
       " 'depois': 130,\n",
       " 'aqui': 132,\n",
       " 'ali': 133,\n",
       " 'lá': 134,\n",
       " 'longe': 135,\n",
       " 'perto': 136,\n",
       " 'escola': 137,\n",
       " 'cidade': 138,\n",
       " 'país': 139,\n",
       " 'terra': 140,\n",
       " 'céu': 141,\n",
       " 'mar': 142,\n",
       " 'praia': 143,\n",
       " 'montanha': 144,\n",
       " 'rio': 145,\n",
       " 'floresta': 146,\n",
       " 'livraria': 147,\n",
       " 'mercado': 148,\n",
       " 'amigo': 149,\n",
       " 'amiga': 150,\n",
       " 'criança': 151,\n",
       " 'professor': 152,\n",
       " 'aluno': 153,\n",
       " 'pessoa': 155,\n",
       " 'família': 156,\n",
       " 'história': 157,\n",
       " 'amor': 158,\n",
       " 'vou': 159,\n",
       " 'vai': 160,\n",
       " 'vamos': 161,\n",
       " 'vão': 162,\n",
       " 'fui': 163,\n",
       " 'foi': 164,\n",
       " 'foram': 165,\n",
       " 'estou': 166,\n",
       " 'está': 167,\n",
       " 'estamos': 168,\n",
       " 'estão': 169,\n",
       " 'gostar': 170,\n",
       " 'amar': 171,\n",
       " 'odiar': 172,\n",
       " 'correr': 173,\n",
       " 'andar': 174,\n",
       " 'viajar': 175,\n",
       " 'sorrir': 176,\n",
       " 'chorar': 177,\n",
       " 'brincar': 178,\n",
       " 'estudar': 179,\n",
       " 'beber': 180,\n",
       " 'comer': 181,\n",
       " 'dormir': 182,\n",
       " 'acordar': 183,\n",
       " 'abrir': 184,\n",
       " 'fechar': 185,\n",
       " 'comprar': 186,\n",
       " 'vender': 187,\n",
       " 'ligar': 188,\n",
       " 'desligar': 189,\n",
       " 'e': 190,\n",
       " 'mas': 191,\n",
       " 'ou': 192,\n",
       " 'porque': 193,\n",
       " 'que': 194,\n",
       " 'como': 195,\n",
       " 'quando': 196,\n",
       " 'onde': 197,\n",
       " 'quem': 198,\n",
       " 'qual': 199}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokens_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec0e9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome: wte.weight\n",
      "Shape: torch.Size([200, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[-1.9411e-01,  2.3504e-01, -4.9729e-01,  ..., -8.0626e-01,\n",
      "          8.2647e-01,  3.5054e-01],\n",
      "        [-1.0369e+00, -8.7673e-01,  5.5250e-03,  ..., -7.8883e-01,\n",
      "          1.1758e-01, -9.1098e-03],\n",
      "        [-3.4526e-01,  1.6308e-01,  6.6631e-01,  ..., -9.3561e-01,\n",
      "          2.6943e-01, -1.2901e-03],\n",
      "        ...,\n",
      "        [-7.7874e-01, -5.6155e-01,  1.7890e-01,  ..., -9.3923e-01,\n",
      "          1.6842e+00,  1.7876e+00],\n",
      "        [ 1.3884e+00, -8.5188e-01, -9.9703e-01,  ...,  1.6953e+00,\n",
      "         -3.8701e-01,  7.8332e-01],\n",
      "        [-1.7553e-01,  1.9899e-01,  9.0632e-01,  ..., -1.7981e-01,\n",
      "         -2.8316e+00, -1.1663e+00]], requires_grad=True)\n",
      "\n",
      "Nome: wpe.weight\n",
      "Shape: torch.Size([64, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 2.1085,  1.0800, -0.4896,  ..., -0.4711,  0.5440, -0.7357],\n",
      "        [-2.0055, -1.5082,  1.0819,  ...,  0.6395, -1.3682,  0.4735],\n",
      "        [-1.2321, -0.6910, -0.0711,  ..., -0.1204,  0.6378,  0.0183],\n",
      "        ...,\n",
      "        [-0.4650,  0.9147, -0.3607,  ..., -0.2991, -1.7426,  0.9233],\n",
      "        [ 0.4356, -0.6125, -0.6825,  ..., -3.4943,  0.4841,  0.2756],\n",
      "        [-0.3978,  0.0874, -1.6223,  ..., -2.3217, -0.8107,  0.2346]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc1.weight\n",
      "Shape: torch.Size([32, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[-0.1675, -0.0939,  0.0702,  0.2467, -0.1176, -0.0018,  0.1721, -0.1971,\n",
      "         -0.0357,  0.1777,  0.2166, -0.1478, -0.1353, -0.2431, -0.1677,  0.2435],\n",
      "        [ 0.0684, -0.0192, -0.0222,  0.0192,  0.0222,  0.0107,  0.0553,  0.2085,\n",
      "         -0.0444,  0.0602,  0.0150, -0.0837,  0.2151, -0.0767,  0.1883,  0.1849],\n",
      "        [ 0.0172,  0.1139, -0.0396, -0.1342, -0.2034,  0.0248, -0.1227,  0.1218,\n",
      "          0.1168, -0.0259,  0.1417, -0.1724, -0.2077, -0.0258, -0.0335, -0.1649],\n",
      "        [-0.2131, -0.0558, -0.2106, -0.0744, -0.1050,  0.2304,  0.1096,  0.1631,\n",
      "          0.2095, -0.2284, -0.1209, -0.0330, -0.0348, -0.1358,  0.1942,  0.2076],\n",
      "        [ 0.1284, -0.1818,  0.2448,  0.0187, -0.0660, -0.2461,  0.2258,  0.0431,\n",
      "          0.1311, -0.0813, -0.1642,  0.1237, -0.1066, -0.2127, -0.1627,  0.2379],\n",
      "        [ 0.0924, -0.2399,  0.0193, -0.1037, -0.1948,  0.2097,  0.1316,  0.1772,\n",
      "         -0.1234,  0.2275, -0.0386,  0.1123,  0.1663, -0.1863,  0.0513, -0.1585],\n",
      "        [-0.0375, -0.1044, -0.0802,  0.0949, -0.2308,  0.1965, -0.2376, -0.2037,\n",
      "         -0.1133, -0.1694,  0.2465, -0.0464, -0.1549,  0.1455, -0.0257,  0.1227],\n",
      "        [-0.0761,  0.1297,  0.0559,  0.2192, -0.1312, -0.0475, -0.0667, -0.1011,\n",
      "         -0.1744,  0.0212, -0.2098,  0.0687,  0.2355,  0.1489,  0.1396, -0.0519],\n",
      "        [-0.0886,  0.1319,  0.1531, -0.1697, -0.0111,  0.0826,  0.1619, -0.2341,\n",
      "         -0.0238,  0.2443, -0.1608, -0.1736,  0.0638,  0.0080,  0.1298,  0.0147],\n",
      "        [-0.1400,  0.0946,  0.0613, -0.1481, -0.1746,  0.0207,  0.2318,  0.1995,\n",
      "          0.0351,  0.1414,  0.0481,  0.1626,  0.2375, -0.1459, -0.1046, -0.0697],\n",
      "        [-0.1832,  0.1357,  0.1069,  0.1613, -0.0148,  0.1244,  0.1876, -0.0022,\n",
      "          0.1229,  0.0018,  0.1943, -0.0998,  0.0681, -0.0271, -0.1643, -0.0493],\n",
      "        [ 0.1818, -0.1802, -0.1106, -0.0005,  0.0385,  0.2370, -0.0512,  0.2141,\n",
      "          0.0134, -0.0942,  0.0659,  0.1141, -0.0132, -0.0848,  0.1426,  0.1854],\n",
      "        [ 0.1665, -0.0443, -0.1951,  0.1191,  0.0434, -0.1644,  0.0730,  0.0504,\n",
      "          0.0242, -0.0794, -0.0730,  0.0711, -0.2193,  0.1220,  0.1031, -0.2241],\n",
      "        [-0.1244, -0.1047,  0.0375,  0.0150, -0.1319,  0.1766, -0.2368,  0.2109,\n",
      "         -0.2415,  0.0804,  0.0962, -0.1505, -0.2313, -0.2344,  0.1781, -0.1766],\n",
      "        [ 0.0647, -0.0159, -0.1747,  0.2424,  0.0251, -0.2255,  0.1793,  0.1321,\n",
      "         -0.1169, -0.1064,  0.0373,  0.0828,  0.1351,  0.2286,  0.2464, -0.2083],\n",
      "        [-0.0712,  0.1426, -0.0300, -0.0673,  0.0368, -0.2233, -0.1777,  0.0605,\n",
      "          0.2120, -0.1636,  0.0741,  0.1263,  0.0513,  0.1183, -0.2360,  0.1606],\n",
      "        [-0.0603,  0.2179, -0.0647,  0.1886,  0.0030,  0.0635, -0.1815,  0.0626,\n",
      "          0.0332, -0.1144, -0.2120, -0.1348,  0.1592,  0.2162,  0.1276, -0.0044],\n",
      "        [ 0.1753,  0.2491,  0.0798, -0.0134,  0.0385,  0.1041,  0.1931, -0.0457,\n",
      "          0.1758, -0.0997, -0.1589,  0.1180,  0.1997,  0.1663, -0.1568, -0.0036],\n",
      "        [ 0.0571, -0.0401,  0.0310, -0.1792, -0.2190,  0.1004,  0.0502,  0.1682,\n",
      "          0.2197,  0.2230,  0.2160,  0.0445, -0.0268, -0.0629, -0.2132, -0.1938],\n",
      "        [ 0.2076,  0.1816,  0.1056, -0.0385,  0.1676,  0.1494,  0.2290, -0.2038,\n",
      "         -0.0236,  0.1920, -0.0572,  0.0045, -0.1756, -0.0079, -0.2302, -0.0966],\n",
      "        [-0.1736,  0.2010,  0.2309,  0.2040, -0.0384,  0.2487, -0.1195,  0.0357,\n",
      "         -0.1103, -0.0016,  0.0466, -0.1290,  0.0901, -0.1920, -0.2079, -0.1223],\n",
      "        [-0.2085,  0.0528,  0.1391, -0.0416,  0.0354, -0.2209,  0.0284, -0.0943,\n",
      "         -0.1336,  0.0445,  0.1589, -0.1380, -0.0362, -0.0163,  0.0011, -0.2441],\n",
      "        [ 0.2076, -0.0163,  0.1859,  0.2349, -0.1195,  0.0868, -0.1880, -0.1919,\n",
      "          0.0970,  0.0652,  0.0147,  0.1599, -0.1973, -0.2460,  0.1092,  0.0580],\n",
      "        [ 0.1213, -0.0997,  0.1499, -0.1799,  0.2128,  0.1467,  0.0083, -0.1674,\n",
      "          0.2016,  0.0593,  0.1944,  0.0912, -0.0131, -0.1013,  0.1672, -0.0725],\n",
      "        [-0.0415, -0.0419, -0.1391,  0.0088,  0.1795, -0.1948, -0.2220, -0.0710,\n",
      "          0.2100,  0.2044,  0.0302, -0.2323, -0.1928, -0.2179, -0.0484,  0.1597],\n",
      "        [-0.0436,  0.1207,  0.1470,  0.2083, -0.0324, -0.0496,  0.1618, -0.2015,\n",
      "          0.1441, -0.1917, -0.0970,  0.1525, -0.1981,  0.0952, -0.1812, -0.1019],\n",
      "        [ 0.0137, -0.1501,  0.0230,  0.1735,  0.2279, -0.0724, -0.1114,  0.1949,\n",
      "         -0.0213,  0.1946,  0.0216,  0.0885, -0.1192, -0.1141, -0.1469,  0.2360],\n",
      "        [-0.0047,  0.0355,  0.2045, -0.1255, -0.2255, -0.0232,  0.0729,  0.2231,\n",
      "          0.1740,  0.2246, -0.0440,  0.1215, -0.0121,  0.0609, -0.0262, -0.1593],\n",
      "        [ 0.0178,  0.2127,  0.2171,  0.0440,  0.2167,  0.2287, -0.1910, -0.0626,\n",
      "          0.0953,  0.1177, -0.1688,  0.1945,  0.1110, -0.0960,  0.0718,  0.0088],\n",
      "        [-0.1496,  0.2106,  0.0149, -0.2361,  0.1603,  0.1963, -0.0161, -0.2351,\n",
      "         -0.1986, -0.0800,  0.1347,  0.1584, -0.1803, -0.1272,  0.0977,  0.0194],\n",
      "        [-0.1287, -0.2292, -0.1986,  0.1995, -0.1421,  0.2000, -0.0112, -0.0991,\n",
      "          0.0668,  0.1067, -0.2045, -0.0179,  0.0611,  0.0336, -0.1434, -0.1349],\n",
      "        [ 0.0683, -0.1967, -0.0636,  0.1090,  0.2022,  0.0033,  0.0977, -0.0620,\n",
      "          0.1647,  0.1337, -0.0942,  0.0639, -0.0112,  0.1819,  0.2180,  0.1905]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc1.bias\n",
      "Shape: torch.Size([32])\n",
      "Valores: Parameter containing:\n",
      "tensor([-0.1799,  0.0089, -0.1230,  0.0658, -0.2407, -0.0497,  0.0883,  0.2420,\n",
      "        -0.1726,  0.1227, -0.0745, -0.2495,  0.2294,  0.2380, -0.2266, -0.2175,\n",
      "        -0.1727,  0.2305,  0.0021, -0.0521, -0.1377,  0.0550, -0.0787, -0.2058,\n",
      "        -0.1967, -0.0490,  0.1712, -0.2270, -0.1198,  0.0244,  0.2427, -0.0289],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc2.weight\n",
      "Shape: torch.Size([16, 32])\n",
      "Valores: Parameter containing:\n",
      "tensor([[-0.1309, -0.1721,  0.1746, -0.1492,  0.1532, -0.0989, -0.0769,  0.1622,\n",
      "          0.0147, -0.0699, -0.0588, -0.0272,  0.1331,  0.0366, -0.0473, -0.1261,\n",
      "          0.1361,  0.0787, -0.1727,  0.1569, -0.1668,  0.0965, -0.0601,  0.0849,\n",
      "          0.0562,  0.0574, -0.1723, -0.0432, -0.0651, -0.1344, -0.1209,  0.1036],\n",
      "        [-0.0450,  0.0636,  0.0648, -0.1487, -0.0967, -0.0859,  0.0803, -0.0020,\n",
      "          0.1472, -0.0445,  0.1649,  0.1009,  0.1387, -0.0096,  0.1165, -0.0076,\n",
      "         -0.0188, -0.1099, -0.1550, -0.0800,  0.0130,  0.0604,  0.0102, -0.1319,\n",
      "          0.1339,  0.0641, -0.1149, -0.1366,  0.0622, -0.1066,  0.0596,  0.1487],\n",
      "        [-0.1320, -0.1384, -0.1464, -0.1577,  0.0777,  0.0273, -0.0685, -0.0037,\n",
      "          0.0093,  0.0843, -0.0018, -0.0273, -0.1021,  0.1350,  0.0994,  0.0243,\n",
      "         -0.1299,  0.0569,  0.0204,  0.0448, -0.0238, -0.1395,  0.1196, -0.0460,\n",
      "          0.0429,  0.1348, -0.0497,  0.0412, -0.0536,  0.0945, -0.0967, -0.1228],\n",
      "        [-0.1493, -0.0545, -0.1693, -0.0297,  0.1640, -0.0423, -0.0496,  0.1091,\n",
      "         -0.1202, -0.1335,  0.1656, -0.0895,  0.0833, -0.0403, -0.0848, -0.1497,\n",
      "          0.1024,  0.0435, -0.1540,  0.0665,  0.1504,  0.0985, -0.1264, -0.1214,\n",
      "         -0.1328, -0.1741,  0.1015,  0.0347,  0.0781, -0.1322, -0.1523,  0.0198],\n",
      "        [-0.1585, -0.0633,  0.1036,  0.1331,  0.1136, -0.1276,  0.1377, -0.1219,\n",
      "          0.0923,  0.0578,  0.0779,  0.0830, -0.1012,  0.1276, -0.0207, -0.0188,\n",
      "         -0.1462,  0.0787,  0.0661,  0.0285,  0.1051, -0.0219,  0.1183,  0.0605,\n",
      "         -0.0468, -0.0756,  0.1686, -0.0161,  0.0372,  0.1659,  0.0329, -0.0574],\n",
      "        [ 0.1617, -0.0346,  0.0098, -0.0311,  0.0856,  0.1500, -0.0877, -0.1230,\n",
      "          0.0770, -0.1345,  0.0262,  0.0321, -0.0081,  0.1019,  0.1065,  0.0246,\n",
      "         -0.1166, -0.1553, -0.0575, -0.0894,  0.0085,  0.1387, -0.1186,  0.0397,\n",
      "          0.1364,  0.1213,  0.0264,  0.0149, -0.0672,  0.0895,  0.0920,  0.0002],\n",
      "        [ 0.0586, -0.0417, -0.1682, -0.1441,  0.0222, -0.0466,  0.1579,  0.0768,\n",
      "         -0.1244, -0.0491,  0.0704, -0.1484, -0.1453,  0.1139, -0.0547, -0.0736,\n",
      "         -0.0470,  0.0156, -0.1553,  0.1161, -0.1551, -0.1072, -0.0584, -0.1110,\n",
      "         -0.0006,  0.0220, -0.1178, -0.0548, -0.0873, -0.0166,  0.0246, -0.0780],\n",
      "        [ 0.0336,  0.1343, -0.0885,  0.1646,  0.1177,  0.0893, -0.1504, -0.0082,\n",
      "          0.1024,  0.0255,  0.0640, -0.1688, -0.0713,  0.1562,  0.1155,  0.0680,\n",
      "          0.0697, -0.0423, -0.1661,  0.1308,  0.0028,  0.0990, -0.0575,  0.1377,\n",
      "          0.1090,  0.0463, -0.1427,  0.0476, -0.1759, -0.1027, -0.0983,  0.1158],\n",
      "        [-0.0274,  0.0998, -0.0706,  0.0392, -0.1033, -0.0018,  0.0383,  0.1644,\n",
      "          0.0844,  0.0615, -0.0400, -0.1304, -0.0167,  0.1345,  0.0790, -0.1264,\n",
      "          0.1324, -0.0599,  0.0570, -0.0568,  0.0697, -0.0464, -0.0359,  0.0302,\n",
      "          0.1318, -0.0583,  0.0146, -0.0398, -0.1535,  0.0146, -0.1632, -0.1539],\n",
      "        [-0.0357,  0.1419, -0.0967,  0.0078, -0.1705,  0.0216,  0.1074, -0.0611,\n",
      "          0.0452, -0.1309, -0.0412, -0.1507,  0.0362, -0.1171, -0.0547,  0.1670,\n",
      "         -0.0946,  0.1240, -0.1295,  0.1323, -0.1472,  0.0405,  0.1496,  0.1491,\n",
      "          0.0051,  0.1129, -0.0993, -0.0603,  0.0879,  0.0643,  0.1655, -0.1299],\n",
      "        [-0.0089,  0.0016, -0.1286,  0.0988, -0.0194, -0.1056,  0.0774,  0.0670,\n",
      "          0.0360, -0.0462, -0.1331, -0.0194,  0.1047, -0.1739,  0.1059,  0.1183,\n",
      "         -0.0930,  0.0878,  0.0925,  0.0460,  0.0375,  0.1522, -0.1018, -0.0597,\n",
      "         -0.1489,  0.1278, -0.0722,  0.1694,  0.0763, -0.0086,  0.1111, -0.0296],\n",
      "        [ 0.0499,  0.1592, -0.1358,  0.1084,  0.0062,  0.0599, -0.1653,  0.0271,\n",
      "         -0.1545, -0.1587,  0.1075,  0.0296,  0.1739,  0.0675, -0.0772, -0.0602,\n",
      "         -0.0146, -0.0718, -0.1028,  0.1374,  0.0655,  0.0177, -0.0077, -0.1598,\n",
      "         -0.1739,  0.0543,  0.1328, -0.0991, -0.1256, -0.0839, -0.1379, -0.0485],\n",
      "        [ 0.1613,  0.0371, -0.0473,  0.1563, -0.0825,  0.1725, -0.0961,  0.0036,\n",
      "         -0.0670,  0.1298,  0.0948,  0.1619, -0.1174,  0.0987,  0.0232, -0.1745,\n",
      "          0.0426,  0.1271, -0.0206,  0.1172,  0.0261, -0.1140, -0.0838, -0.0721,\n",
      "         -0.0754, -0.1499, -0.1097, -0.0755,  0.1002, -0.1406,  0.0815, -0.1700],\n",
      "        [ 0.1686,  0.1018,  0.1091, -0.1051,  0.0485,  0.1380,  0.1030,  0.1157,\n",
      "          0.0016, -0.0534,  0.1046, -0.1425,  0.1273, -0.1296,  0.0077, -0.0911,\n",
      "         -0.1622, -0.1146,  0.1159,  0.1400, -0.0329,  0.1508,  0.1154,  0.0820,\n",
      "         -0.1742, -0.0009, -0.0799,  0.0459, -0.1535,  0.1099, -0.0051,  0.1211],\n",
      "        [-0.1460, -0.1598,  0.0012, -0.0396, -0.1019,  0.0972, -0.0313,  0.0386,\n",
      "         -0.1243,  0.0844,  0.1046,  0.0834,  0.1263, -0.1767,  0.1004, -0.1434,\n",
      "          0.1042,  0.1288,  0.0654, -0.1453, -0.1260,  0.0061,  0.0906, -0.1369,\n",
      "          0.1149, -0.0229, -0.1151,  0.1603,  0.1679, -0.0698,  0.1385, -0.0066],\n",
      "        [-0.1106,  0.1037,  0.0543,  0.1587,  0.0719,  0.1501, -0.0239, -0.0188,\n",
      "         -0.1286, -0.1706,  0.0746,  0.0703, -0.0027, -0.0944,  0.0050, -0.0504,\n",
      "         -0.0083,  0.0350, -0.1323,  0.0676,  0.0644,  0.1008, -0.1128, -0.1563,\n",
      "          0.0971,  0.1542, -0.0393,  0.0305, -0.0151, -0.0061,  0.0787, -0.0352]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc2.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([ 0.1397, -0.0677,  0.0850, -0.0427,  0.0771, -0.0704, -0.0513,  0.0402,\n",
      "         0.0506, -0.0092, -0.0711, -0.0116,  0.0728, -0.0860,  0.1280,  0.1518],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: ln.weight\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "Nome: ln.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Nome: qkv_proj.weight\n",
      "Shape: torch.Size([48, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 0.2351, -0.0803,  0.0277,  0.0587, -0.1100,  0.1653,  0.2061, -0.0772,\n",
      "          0.2119, -0.0043, -0.0885,  0.1408, -0.2405,  0.2227,  0.0630, -0.2001],\n",
      "        [-0.1267,  0.2077, -0.1404, -0.0072,  0.2094, -0.1562, -0.2414,  0.0900,\n",
      "          0.1413,  0.2305, -0.2144,  0.1779,  0.2023,  0.0104,  0.2234, -0.1909],\n",
      "        [-0.1190, -0.2492, -0.0373,  0.2270, -0.1611,  0.1787, -0.1225,  0.0658,\n",
      "         -0.1191,  0.1440,  0.1082, -0.2352,  0.1166, -0.0993,  0.0619, -0.0771],\n",
      "        [-0.1145, -0.1782,  0.0776,  0.1579,  0.1940,  0.0219, -0.0578, -0.1483,\n",
      "         -0.1728, -0.2352,  0.2303,  0.0591,  0.1567,  0.0370, -0.2129, -0.1385],\n",
      "        [ 0.0822, -0.1347,  0.1769,  0.2421,  0.1070, -0.2486, -0.2333,  0.1953,\n",
      "          0.0777,  0.1545, -0.1351, -0.0382, -0.0561,  0.0425, -0.0852, -0.1204],\n",
      "        [-0.1150, -0.0641, -0.2236, -0.1371,  0.1275,  0.1223, -0.0412,  0.2355,\n",
      "         -0.0546,  0.1984,  0.0206, -0.0577, -0.1385, -0.1587,  0.0185,  0.0992],\n",
      "        [-0.0101,  0.2006,  0.0369, -0.1751, -0.0072,  0.1669, -0.2409,  0.2174,\n",
      "         -0.0340, -0.1259,  0.1032,  0.2123, -0.2010,  0.1496,  0.1218, -0.0162],\n",
      "        [ 0.1655,  0.1303, -0.2074,  0.0954, -0.0205, -0.0781, -0.0433,  0.0805,\n",
      "         -0.1181,  0.1026,  0.0716,  0.1609, -0.1365, -0.0144,  0.2303,  0.1885],\n",
      "        [ 0.0011, -0.0765, -0.0550,  0.2256,  0.1802,  0.1031,  0.0495, -0.2344,\n",
      "         -0.0496, -0.0539, -0.2153,  0.0741,  0.1089,  0.0207,  0.2137,  0.1846],\n",
      "        [ 0.0977,  0.2407, -0.0851,  0.0978,  0.0837, -0.2151, -0.0140, -0.1277,\n",
      "          0.0289,  0.0332,  0.2350,  0.0856,  0.0444, -0.0869, -0.1642,  0.0952],\n",
      "        [ 0.1332,  0.0306,  0.1059, -0.0943,  0.1852, -0.0899, -0.0519, -0.1226,\n",
      "         -0.0433, -0.1803,  0.2012,  0.1947,  0.0525,  0.2480, -0.1563, -0.1348],\n",
      "        [ 0.2002,  0.0371, -0.0251,  0.0214, -0.0197,  0.1988, -0.2310,  0.1522,\n",
      "          0.2451, -0.0556,  0.1315,  0.0588, -0.2114,  0.0545, -0.1590,  0.1626],\n",
      "        [ 0.1482, -0.1957, -0.1162, -0.0433, -0.1958,  0.1327, -0.1925,  0.0178,\n",
      "          0.0242, -0.1466,  0.1588,  0.1864,  0.0717, -0.1702,  0.0960,  0.2160],\n",
      "        [-0.1274, -0.0856, -0.1123,  0.2103,  0.1644,  0.2063,  0.0386, -0.1690,\n",
      "         -0.1449,  0.0138,  0.1574, -0.0762, -0.0606,  0.0593, -0.2382, -0.1223],\n",
      "        [-0.1264, -0.1008,  0.0930, -0.1793,  0.1911,  0.0650, -0.1614, -0.1223,\n",
      "          0.0365,  0.1150, -0.1686, -0.0336, -0.2221, -0.2450, -0.0087,  0.1864],\n",
      "        [-0.0039,  0.1510, -0.2110, -0.0907,  0.0489,  0.1607, -0.0843,  0.1661,\n",
      "         -0.0859,  0.0310, -0.2391,  0.2063, -0.0470, -0.1481, -0.1462, -0.0636],\n",
      "        [-0.0963,  0.2024, -0.1868, -0.2328, -0.0028,  0.1942, -0.1311,  0.0247,\n",
      "         -0.1784, -0.1766, -0.1364, -0.0701, -0.0945, -0.1157,  0.2284, -0.1506],\n",
      "        [ 0.1898,  0.1033, -0.1667, -0.1124, -0.2465,  0.0552, -0.1763,  0.1944,\n",
      "          0.1868, -0.0765, -0.0414,  0.1954, -0.0223,  0.1849,  0.0005,  0.0382],\n",
      "        [ 0.0900, -0.0132,  0.2080,  0.2139,  0.1491,  0.2354,  0.1034,  0.2491,\n",
      "          0.0677, -0.1699, -0.0128, -0.0061, -0.1594, -0.0502, -0.2311,  0.1771],\n",
      "        [ 0.0165, -0.1878, -0.1755,  0.2221, -0.0525, -0.1260,  0.2209, -0.1995,\n",
      "          0.2307, -0.1210, -0.2398,  0.1561,  0.0824,  0.2303, -0.1995,  0.0330],\n",
      "        [-0.1369, -0.2430, -0.2229, -0.0470,  0.0842,  0.0438, -0.1159, -0.2000,\n",
      "         -0.1866, -0.2042,  0.1259,  0.0490,  0.2406,  0.0831, -0.0734, -0.1111],\n",
      "        [-0.2452, -0.0893,  0.1313, -0.0792,  0.1043, -0.1898, -0.1305,  0.0430,\n",
      "          0.1037,  0.1894,  0.0377, -0.1425,  0.1041,  0.0556, -0.1215, -0.1952],\n",
      "        [ 0.0789, -0.0347,  0.2125,  0.1952,  0.1087,  0.0286,  0.2173,  0.0343,\n",
      "          0.2081,  0.0312,  0.0303, -0.0918,  0.0028,  0.2030, -0.2044, -0.1675],\n",
      "        [ 0.2280,  0.1367, -0.1365, -0.2246,  0.1240, -0.2049,  0.1984, -0.0890,\n",
      "         -0.0538,  0.0054, -0.0795, -0.1680,  0.0223,  0.0538,  0.2009,  0.0376],\n",
      "        [ 0.1694, -0.0080,  0.0934, -0.2329, -0.0665,  0.1311,  0.1074, -0.0392,\n",
      "          0.0241,  0.0347,  0.0370,  0.1804, -0.1014,  0.2237, -0.1766,  0.2326],\n",
      "        [ 0.1329,  0.0270,  0.0688, -0.2280,  0.1312, -0.1010,  0.0652, -0.1927,\n",
      "         -0.0894, -0.1116,  0.1429, -0.1323, -0.2324, -0.1939, -0.0758,  0.0626],\n",
      "        [ 0.0804, -0.0765, -0.0648, -0.1987,  0.0370,  0.0827, -0.0114, -0.1458,\n",
      "         -0.2292,  0.0059,  0.0566, -0.1190,  0.1981,  0.1123,  0.0123, -0.0562],\n",
      "        [-0.2245, -0.2295, -0.0344,  0.1141, -0.0094,  0.0754,  0.2189,  0.0355,\n",
      "         -0.1535,  0.0569, -0.0397,  0.0904, -0.0511, -0.1666,  0.0240, -0.1671],\n",
      "        [-0.0394,  0.1984,  0.0689,  0.2331,  0.0495,  0.0568,  0.0601, -0.0434,\n",
      "          0.2052, -0.0387,  0.1149,  0.0565, -0.1592, -0.1167,  0.1425, -0.0825],\n",
      "        [ 0.1138,  0.2469,  0.1125, -0.0616,  0.0353, -0.0817,  0.1486,  0.0109,\n",
      "          0.0457, -0.2364, -0.1931, -0.1496, -0.0091,  0.0863,  0.1883,  0.0100],\n",
      "        [-0.1792,  0.1632,  0.0625,  0.2355,  0.0360, -0.0717, -0.2053, -0.0443,\n",
      "          0.1338, -0.0120, -0.2388,  0.1792, -0.2389, -0.0985, -0.0546,  0.1401],\n",
      "        [-0.2301, -0.2278,  0.0481,  0.1989, -0.1745, -0.1302, -0.1454,  0.0608,\n",
      "         -0.0084, -0.1229,  0.1618,  0.0647,  0.1105, -0.0667,  0.1412, -0.0521],\n",
      "        [ 0.0945,  0.0197, -0.0449, -0.1002,  0.1248, -0.1426,  0.1144, -0.0436,\n",
      "          0.0618,  0.1178, -0.2150,  0.2207, -0.1550, -0.2334, -0.1312, -0.0356],\n",
      "        [ 0.1811,  0.2163,  0.0106,  0.0006, -0.1220,  0.0462,  0.0198, -0.2147,\n",
      "         -0.1000, -0.0694, -0.1105,  0.2046,  0.1671, -0.2149,  0.0740, -0.0790],\n",
      "        [ 0.1386,  0.1396, -0.1718,  0.0560,  0.2260,  0.0448,  0.0422,  0.0742,\n",
      "         -0.0007, -0.1555, -0.1353, -0.2276, -0.2127, -0.1730, -0.1754,  0.2012],\n",
      "        [ 0.1726,  0.1625, -0.1684, -0.0689,  0.0085, -0.1279, -0.0873, -0.1962,\n",
      "          0.1508, -0.0754,  0.2096,  0.1675,  0.1941,  0.1560, -0.2038,  0.2066],\n",
      "        [-0.1060, -0.1084,  0.0857,  0.1054, -0.1165, -0.0253,  0.0174, -0.1540,\n",
      "         -0.0042, -0.0102,  0.0380,  0.0965, -0.0322,  0.1347,  0.1508,  0.1252],\n",
      "        [-0.2169,  0.0197, -0.0357, -0.1645, -0.2029,  0.0843, -0.0752, -0.2073,\n",
      "          0.0835,  0.0409, -0.1857,  0.0652, -0.0909,  0.1331, -0.0036,  0.2143],\n",
      "        [ 0.0244, -0.1670,  0.2497, -0.1671, -0.2393, -0.0092, -0.0910, -0.2418,\n",
      "          0.2400, -0.1110, -0.2174, -0.1893, -0.1627, -0.0235, -0.2275,  0.0551],\n",
      "        [-0.1374, -0.0801, -0.0072, -0.1110, -0.0179, -0.2074,  0.1044,  0.2061,\n",
      "         -0.0359,  0.0046,  0.1975, -0.2213,  0.1597,  0.1118,  0.0457,  0.0315],\n",
      "        [-0.0239, -0.2316, -0.0873, -0.2225,  0.1373, -0.1457, -0.0460, -0.0886,\n",
      "          0.1302,  0.1184, -0.0865,  0.0651,  0.1405, -0.1432, -0.2410,  0.0242],\n",
      "        [ 0.0997,  0.0174,  0.0593,  0.0094,  0.1860, -0.1070, -0.1894,  0.0004,\n",
      "          0.0402,  0.1616,  0.1757,  0.2348,  0.0757,  0.0233, -0.1347, -0.0372],\n",
      "        [-0.0304, -0.0827,  0.0737,  0.2437,  0.2298, -0.2350, -0.0066, -0.0688,\n",
      "          0.0108, -0.2257,  0.2181,  0.2188, -0.0725,  0.1404,  0.1084, -0.0959],\n",
      "        [-0.2064,  0.0805,  0.1216,  0.1667,  0.2452,  0.0500, -0.1544,  0.1685,\n",
      "         -0.0210, -0.1892, -0.0024,  0.2226, -0.0723,  0.1944, -0.1312,  0.2045],\n",
      "        [ 0.0385,  0.2452,  0.0738, -0.1961, -0.2279,  0.1302, -0.0278, -0.1278,\n",
      "          0.2434,  0.0301,  0.0964,  0.1870,  0.2422,  0.1296,  0.0762, -0.1102],\n",
      "        [-0.0518,  0.2120, -0.1926,  0.1009,  0.0394,  0.0294,  0.1137,  0.2445,\n",
      "          0.0807, -0.0708,  0.1425, -0.1176, -0.1798,  0.1673, -0.2043, -0.1114],\n",
      "        [-0.1557,  0.1445, -0.1318, -0.2016,  0.0318, -0.1073,  0.1121,  0.2269,\n",
      "         -0.1177,  0.0199, -0.2203,  0.2168,  0.0601, -0.0348, -0.0668, -0.1144],\n",
      "        [-0.2023,  0.1779, -0.0034, -0.2236, -0.1686,  0.0972,  0.2189,  0.1355,\n",
      "          0.1466,  0.0361,  0.0288,  0.1645, -0.0917,  0.2088,  0.1744,  0.1847]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: qkv_proj.bias\n",
      "Shape: torch.Size([48])\n",
      "Valores: Parameter containing:\n",
      "tensor([ 0.0483,  0.2226,  0.0128,  0.1332,  0.0167,  0.0762,  0.2165, -0.1860,\n",
      "        -0.0958, -0.2109, -0.1847,  0.1953,  0.1930, -0.0538,  0.0565,  0.1751,\n",
      "        -0.0626, -0.2185, -0.0333,  0.1057, -0.2305,  0.1031, -0.0491,  0.1302,\n",
      "         0.2099, -0.1366,  0.0973, -0.1303, -0.0080,  0.0497, -0.0741,  0.1321,\n",
      "         0.0987, -0.2291,  0.1808,  0.0183,  0.1601, -0.1126, -0.2012, -0.1170,\n",
      "         0.1089, -0.2478, -0.0096,  0.0864, -0.1257, -0.1166, -0.0486, -0.1154],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: out_proj.weight\n",
      "Shape: torch.Size([16, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 0.0547,  0.0714, -0.0814,  0.0796,  0.2003, -0.1515, -0.1135, -0.1256,\n",
      "         -0.0908, -0.0589, -0.0812,  0.0917, -0.0471,  0.0118, -0.1081,  0.1844],\n",
      "        [-0.0442,  0.2394,  0.1857,  0.0078,  0.1927,  0.1149, -0.1021,  0.1508,\n",
      "          0.0831,  0.1409,  0.1045, -0.0945,  0.1819, -0.1943, -0.2357, -0.2164],\n",
      "        [ 0.2410, -0.0699,  0.1547,  0.2488, -0.0182,  0.1718,  0.2385,  0.2421,\n",
      "         -0.0168, -0.2439,  0.0960,  0.2244,  0.1368, -0.2305,  0.1412,  0.0388],\n",
      "        [ 0.0033, -0.2169, -0.0435, -0.0125, -0.0664, -0.0864, -0.0336, -0.1623,\n",
      "         -0.0414, -0.1019, -0.0035, -0.0050, -0.1195, -0.1310, -0.0298, -0.2013],\n",
      "        [ 0.0659, -0.1325, -0.1539,  0.1740, -0.1075, -0.1760, -0.1667, -0.2069,\n",
      "          0.1369, -0.0158, -0.0602, -0.2129,  0.2187,  0.1259, -0.2408, -0.1232],\n",
      "        [-0.0914, -0.2470,  0.1813, -0.0837, -0.0844,  0.0436, -0.1127, -0.2278,\n",
      "         -0.0188, -0.0808,  0.2240, -0.0750,  0.1155, -0.0372,  0.0149,  0.1527],\n",
      "        [-0.0377, -0.1574,  0.0154,  0.0900,  0.1048,  0.0924,  0.0491, -0.1753,\n",
      "          0.1656, -0.0795, -0.0321,  0.0129, -0.2334, -0.0993,  0.1144, -0.1752],\n",
      "        [ 0.0477,  0.0651, -0.0316, -0.0291,  0.1277,  0.0916,  0.0876, -0.1417,\n",
      "          0.0062, -0.1471, -0.1047,  0.1425, -0.2120,  0.2188, -0.2430, -0.0030],\n",
      "        [-0.1468, -0.1247,  0.0620,  0.1404, -0.1976,  0.2398, -0.0849, -0.0585,\n",
      "         -0.1200, -0.0120, -0.0097, -0.0458,  0.0675, -0.0081, -0.2381, -0.1895],\n",
      "        [-0.2169,  0.2039,  0.1080,  0.1362,  0.1825,  0.1227,  0.1370,  0.1657,\n",
      "          0.1796, -0.2374, -0.1068, -0.1121, -0.0559, -0.2139,  0.0088,  0.1095],\n",
      "        [-0.0685,  0.2117,  0.1107, -0.0693,  0.1118,  0.0270, -0.1426, -0.0887,\n",
      "         -0.0328, -0.1302,  0.1933,  0.1375, -0.1300, -0.2058,  0.0058,  0.2180],\n",
      "        [ 0.2433, -0.2431, -0.1811, -0.1832,  0.0449,  0.0883,  0.1275,  0.0016,\n",
      "          0.0296,  0.0347, -0.1192,  0.2029,  0.2307, -0.1200,  0.0056, -0.2142],\n",
      "        [ 0.2161,  0.0084,  0.1206,  0.2348,  0.0471,  0.1519, -0.1070, -0.0349,\n",
      "          0.2282,  0.1391,  0.0825,  0.0279,  0.1338,  0.1742,  0.1613,  0.1423],\n",
      "        [ 0.1953,  0.0420, -0.0808,  0.0497, -0.0883,  0.1638, -0.1382, -0.0957,\n",
      "          0.1331,  0.1257, -0.2052, -0.0416, -0.0753,  0.2165, -0.0674, -0.0776],\n",
      "        [-0.0661, -0.0553, -0.0168, -0.0963, -0.1386, -0.0366, -0.2005, -0.0919,\n",
      "          0.0278,  0.1420,  0.0469, -0.1761, -0.0843,  0.1047,  0.1817,  0.0434],\n",
      "        [-0.2367,  0.1371,  0.1606,  0.2472, -0.2380,  0.1300,  0.0322, -0.2028,\n",
      "          0.0091, -0.2464, -0.0852, -0.2185,  0.0989, -0.0091, -0.2265,  0.0385]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: out_proj.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([-0.2083, -0.1298,  0.1764, -0.0338,  0.1394,  0.0093,  0.2439,  0.0361,\n",
      "        -0.0170, -0.0684, -0.1685, -0.2162,  0.0796,  0.0685, -0.1788, -0.1779],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: final_ln.weight\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: final_ln.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: lm_head.weight\n",
      "Shape: torch.Size([200, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 0.1375, -0.0624, -0.0446,  ..., -0.0708,  0.2270, -0.0474],\n",
      "        [ 0.0022,  0.2155, -0.2151,  ...,  0.1796, -0.2210,  0.0086],\n",
      "        [-0.0362,  0.0191, -0.2172,  ..., -0.0950, -0.0956,  0.0323],\n",
      "        ...,\n",
      "        [-0.1989,  0.2046, -0.1776,  ...,  0.1138,  0.1054,  0.1008],\n",
      "        [-0.1506,  0.0163, -0.1715,  ...,  0.2014,  0.2074, -0.0780],\n",
      "        [ 0.0296,  0.2030,  0.2390,  ...,  0.2172, -0.0500,  0.2018]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: lm_head.bias\n",
      "Shape: torch.Size([200])\n",
      "Valores: Parameter containing:\n",
      "tensor([-0.0702,  0.1780,  0.0952, -0.2032, -0.2432, -0.1530, -0.0851, -0.0102,\n",
      "         0.2190,  0.1572, -0.1758, -0.0403,  0.1165,  0.2300, -0.2495,  0.0411,\n",
      "        -0.2092, -0.0817, -0.0322,  0.1001,  0.0152, -0.0553,  0.2231,  0.1231,\n",
      "         0.2183,  0.0715, -0.1561,  0.0076, -0.1686,  0.1573,  0.0746,  0.1916,\n",
      "         0.0624, -0.2136, -0.1225,  0.0243,  0.2004, -0.1689, -0.0139, -0.0843,\n",
      "         0.1924,  0.0384, -0.0429, -0.2333, -0.1768, -0.0265,  0.2029, -0.0186,\n",
      "         0.0975, -0.0463,  0.1470, -0.0852, -0.0083,  0.2061, -0.1943,  0.2324,\n",
      "        -0.2336,  0.0654, -0.1756,  0.1913, -0.0861, -0.1347, -0.0106, -0.0241,\n",
      "         0.1141, -0.1481, -0.0078, -0.1402, -0.1053,  0.1507, -0.1168, -0.1114,\n",
      "         0.1538,  0.2356, -0.1344, -0.1562, -0.2010, -0.0755,  0.1253,  0.0286,\n",
      "         0.1844, -0.1991, -0.1742,  0.0954,  0.1267,  0.0111, -0.1369, -0.0031,\n",
      "         0.0046,  0.1678,  0.0998,  0.0637, -0.0457, -0.0675, -0.0243,  0.1801,\n",
      "         0.1159, -0.0896, -0.1991, -0.1501, -0.1751,  0.0560, -0.2006,  0.2329,\n",
      "         0.1159,  0.1860, -0.1113, -0.0344,  0.1164,  0.1519,  0.0216,  0.0905,\n",
      "        -0.1304,  0.0767, -0.1367, -0.0313,  0.1797,  0.2015,  0.1283, -0.2204,\n",
      "         0.2170,  0.2170,  0.1290,  0.1704,  0.0842, -0.1952,  0.1326, -0.0111,\n",
      "         0.2441,  0.2343, -0.2357,  0.1106,  0.0058,  0.0595, -0.0423,  0.1336,\n",
      "        -0.1145, -0.1631, -0.1893,  0.2197, -0.0964, -0.0004,  0.0666,  0.1646,\n",
      "        -0.2052,  0.0309,  0.1942, -0.1271,  0.0400, -0.0837,  0.1252,  0.1608,\n",
      "         0.1594, -0.1671,  0.0933,  0.2164, -0.2283,  0.1016,  0.1377, -0.0011,\n",
      "         0.0999,  0.1566, -0.0763, -0.0434,  0.1099, -0.0075,  0.1198, -0.2432,\n",
      "         0.1828, -0.0900, -0.0575,  0.0149,  0.1693, -0.2096, -0.0536, -0.2062,\n",
      "         0.1821,  0.0751,  0.1692,  0.2071,  0.0845, -0.0350, -0.1243,  0.1409,\n",
      "         0.1716, -0.1076,  0.0998, -0.0894,  0.0135, -0.1028, -0.0428, -0.1436,\n",
      "         0.0398, -0.1069, -0.0848,  0.2135,  0.1468,  0.1389, -0.1737, -0.0117],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Nome: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Valores: {param}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "539ba1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (wte): Embedding(200, 16)\n",
      "  (wpe): Embedding(64, 16)\n",
      "  (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (gelu): GELU(approximate='tanh')\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (qkv_proj): Linear(in_features=16, out_features=48, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (final_ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=16, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d1f33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dizer'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5d72df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: o gato pequeno céu\n",
      "Passo 2: o gato pequeno céu sem\n",
      "Passo 3: o gato pequeno céu sem amar\n",
      "Passo 4: o gato pequeno céu sem amar antes\n",
      "Passo 5: o gato pequeno céu sem amar antes em\n",
      "Passo 6: o gato pequeno céu sem amar antes em dez\n",
      "Passo 7: o gato pequeno céu sem amar antes em dez vão\n",
      "Passo 8: o gato pequeno céu sem amar antes em dez vão casa\n",
      "Passo 9: o gato pequeno céu sem amar antes em dez vão casa não\n",
      "Passo 10: o gato pequeno céu sem amar antes em dez vão casa não eu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'gato',\n",
       " 'pequeno',\n",
       " 'céu',\n",
       " 'sem',\n",
       " 'amar',\n",
       " 'antes',\n",
       " 'em',\n",
       " 'dez',\n",
       " 'vão',\n",
       " 'casa',\n",
       " 'não',\n",
       " 'eu']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50974d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
