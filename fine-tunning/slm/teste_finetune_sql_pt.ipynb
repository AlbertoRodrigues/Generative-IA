{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6387bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e1e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Translate to Portuguese: Hello.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b21655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to Portuguese: Hello. Olá.</s>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1087fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cffb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdaf46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc3b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9a8642a",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tuning (PEFT LoRA) para gerar SQL em Português — CPU-friendly\n",
    "\n",
    "Este trecho adiciona um pipeline **Hugging Face Transformers + PEFT (LoRA)** para treinar um **modelo causal pequeno** a gerar **consultas SQL** a partir de **perguntas em português**.  \n",
    "Projeto pensado para **CPU** (vai ser lento, mas funciona) e com **dataset simples** para prova de conceito.\n",
    "\n",
    "> **Modelo base sugerido:** `facebook/opt-350m` (~350M de parâmetros).  \n",
    "> Você pode trocar por outro causal LM compatível.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90faca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Opcional) Instale as dependências quando rodar localmente\n",
    "# Remova o comentário se precisar instalar\n",
    "# !pip install -U transformers datasets peft accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e87938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2d1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Configurações principais ======\n",
    "MODEL_NAME = \"facebook/opt-350m\"  # ~350M params, roda em CPU (lento)\n",
    "OUTPUT_DIR = \"./opt350m-lora-sql-pt\"\n",
    "MAX_SEQ_LEN = 384\n",
    "SEED = 42\n",
    "\n",
    "# CPU-only\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))  # limite para não travar a máquina\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c70b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defina um esquema simples para orientar o modelo\n",
    "SCHEMA = \"\"\"\n",
    "Tabelas e colunas (dialeto: PostgreSQL):\n",
    "\n",
    "clientes(id, nome, cidade)\n",
    "pedidos(id, cliente_id, data, total)\n",
    "itens_pedido(id, pedido_id, produto_id, quantidade, preco)\n",
    "produtos(id, nome, categoria)\n",
    "\"\"\"\n",
    "\n",
    "# Dataset mínimo: NL em PT -> SQL alvo\n",
    "raw_samples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"pergunta\": \"Liste o nome dos clientes e suas cidades.\",\n",
    "        \"sql\": \"SELECT nome, cidade FROM clientes;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Conte quantos clientes existem por cidade.\",\n",
    "        \"sql\": \"SELECT cidade, COUNT(*) AS total_clientes FROM clientes GROUP BY cidade ORDER BY total_clientes DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Quais pedidos têm valor total acima de 1000? Retorne id e total.\",\n",
    "        \"sql\": \"SELECT id, total FROM pedidos WHERE total > 1000;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Liste o total de vendas por mês (YYYY-MM).\",\n",
    "        \"sql\": \"SELECT TO_CHAR(data, 'YYYY-MM') AS mes, SUM(total) AS total_vendas FROM pedidos GROUP BY mes ORDER BY mes;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Quais são os 5 produtos mais vendidos em quantidade?\",\n",
    "        \"sql\": \"SELECT p.nome, SUM(i.quantidade) AS qtd FROM itens_pedido i JOIN produtos p ON p.id = i.produto_id GROUP BY p.nome ORDER BY qtd DESC LIMIT 5;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Qual é o faturamento total por categoria de produto?\",\n",
    "        \"sql\": \"SELECT p.categoria, SUM(i.quantidade * i.preco) AS faturamento FROM itens_pedido i JOIN produtos p ON p.id = i.produto_id GROUP BY p.categoria ORDER BY faturamento DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Liste os pedidos feitos por clientes da cidade de São Paulo.\",\n",
    "        \"sql\": \"SELECT pe.* FROM pedidos pe JOIN clientes c ON c.id = pe.cliente_id WHERE c.cidade = 'São Paulo';\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Para cada cliente, quantos pedidos foram realizados?\",\n",
    "        \"sql\": \"SELECT c.nome, COUNT(pe.id) AS num_pedidos FROM clientes c LEFT JOIN pedidos pe ON pe.cliente_id = c.id GROUP BY c.nome ORDER BY num_pedidos DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Liste os produtos que nunca foram vendidos.\",\n",
    "        \"sql\": \"SELECT p.* FROM produtos p LEFT JOIN itens_pedido i ON i.produto_id = p.id WHERE i.id IS NULL;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Qual é o ticket médio por pedido?\",\n",
    "        \"sql\": \"SELECT AVG(total) AS ticket_medio FROM pedidos;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Liste os pedidos e o nome do cliente correspondente.\",\n",
    "        \"sql\": \"SELECT pe.id, pe.data, pe.total, c.nome FROM pedidos pe JOIN clientes c ON c.id = pe.cliente_id;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Qual é a receita total do sistema?\",\n",
    "        \"sql\": \"SELECT SUM(total) AS receita_total FROM pedidos;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Mostre o top 3 clientes com maior valor total em pedidos.\",\n",
    "        \"sql\": \"SELECT c.nome, SUM(pe.total) AS gasto FROM clientes c JOIN pedidos pe ON pe.cliente_id = c.id GROUP BY c.nome ORDER BY gasto DESC LIMIT 3;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Quais produtos pertencem à categoria 'Eletrônicos'?\",\n",
    "        \"sql\": \"SELECT * FROM produtos WHERE categoria = 'Eletrônicos';\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Liste os pedidos feitos no ano de 2024.\",\n",
    "        \"sql\": \"SELECT * FROM pedidos WHERE EXTRACT(YEAR FROM data) = 2024;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Para cada mês, quantos pedidos foram realizados?\",\n",
    "        \"sql\": \"SELECT TO_CHAR(data, 'YYYY-MM') AS mes, COUNT(*) AS num_pedidos FROM pedidos GROUP BY mes ORDER BY mes;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Qual cliente realizou mais pedidos?\",\n",
    "        \"sql\": \"SELECT c.nome, COUNT(pe.id) AS num_pedidos FROM clientes c JOIN pedidos pe ON pe.cliente_id = c.id GROUP BY c.nome ORDER BY num_pedidos DESC LIMIT 1;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Quais produtos tiveram faturamento acima de 5000?\",\n",
    "        \"sql\": \"SELECT p.nome, SUM(i.quantidade * i.preco) AS faturamento FROM itens_pedido i JOIN produtos p ON p.id = i.produto_id GROUP BY p.nome HAVING SUM(i.quantidade * i.preco) > 5000 ORDER BY faturamento DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Liste o total de itens vendidos por produto no ano de 2023.\",\n",
    "        \"sql\": \"SELECT p.nome, SUM(i.quantidade) AS total_itens FROM itens_pedido i JOIN produtos p ON p.id = i.produto_id JOIN pedidos pe ON pe.id = i.pedido_id WHERE EXTRACT(YEAR FROM pe.data) = 2023 GROUP BY p.nome ORDER BY total_itens DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"Quais clientes nunca realizaram pedidos?\",\n",
    "        \"sql\": \"SELECT c.* FROM clientes c LEFT JOIN pedidos pe ON pe.cliente_id = c.id WHERE pe.id IS NULL;\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Formatação estilo \"instrução\" + \"resposta\" para causal LM\n",
    "INSTRUCTION_TMPL = \"\"\"### Tarefa\n",
    "Escreva a consulta SQL em {dialeto} correspondente à pergunta.\n",
    "\n",
    "### Esquema\n",
    "{schema}\n",
    "\n",
    "### Pergunta\n",
    "{pergunta}\n",
    "\n",
    "### Consulta SQL\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b95666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_example(ex: Dict[str, str]) -> Dict[str, str]:\n",
    "    prompt = INSTRUCTION_TMPL.format(dialeto=\"PostgreSQL\", schema=SCHEMA.strip(), pergunta=ex[\"pergunta\"].strip())\n",
    "    target = ex[\"sql\"].strip()\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"target\": target,\n",
    "        \"text\": prompt + \"\\n\" + target  # útil para debug\n",
    "    }\n",
    "\n",
    "dataset = Dataset.from_list([format_example(s) for s in raw_samples])\n",
    "# Para demo simples, vamos usar todo mundo como treino\n",
    "ds = DatasetDict({\"train\": dataset})\n",
    "len(ds[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7841d4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alber\\.cache\\huggingface\\hub\\models--facebook--opt-350m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449df8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20/20 [00:00<00:00, 361.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_mask(batch):\n",
    "    # tokeniza prompt e prompt+target para mascarar corretamente\n",
    "    prompts = batch[\"prompt\"]\n",
    "    targets = batch[\"target\"]\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for p, t in zip(prompts, targets):\n",
    "        # tokens do prompt (sem o SQL)\n",
    "        tok_prompt = tokenizer(\n",
    "            p,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=False,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # tokens do prompt + target (SQL)\n",
    "        full_txt = p + \"\\n\" + t\n",
    "        tok_full = tokenizer(\n",
    "            full_txt,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        input_ids = tok_full[\"input_ids\"]\n",
    "        attention_mask = tok_full[\"attention_mask\"]\n",
    "\n",
    "        # Máscara: tudo que pertence ao prompt vira -100; o restante (SQL) é treinado\n",
    "        labels = input_ids.copy()\n",
    "        prompt_len = len(tok_prompt[\"input_ids\"])\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(labels)\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"labels\": labels_list,\n",
    "        \"attention_mask\": attention_masks,\n",
    "    }\n",
    "\n",
    "tokenized = ds[\"train\"].map(tokenize_and_mask, batched=True, remove_columns=[\"prompt\",\"target\",\"text\"])\n",
    "tokenized = tokenized.shuffle(seed=SEED)\n",
    "tokenized[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c1e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Carrega modelo base em CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afaaeca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"
     ]
    }
   ],
   "source": [
    "# Configura LoRA (atenção do OPT usa q_proj/v_proj)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab74853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,  # simula batch 16 em CPU\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,              # para demo; aumente em casos reais\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a309be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.lm_head.weight', 'base_model.model.model.decoder.embed_tokens.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m      4\u001b[39m     train_dataset=tokenized,\n\u001b[32m      5\u001b[39m     data_collator=default_data_collator,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\transformers\\trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\transformers\\trainer.py:2623\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2621\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2622\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2634\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\transformers\\trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\transformers\\trainer.py:3200\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3198\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3199\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3203\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\transformers\\trainer.py:3937\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3934\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3936\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3937\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3939\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\transformers\\trainer.py:4035\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4033\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_safetensors:\n\u001b[32m-> \u001b[39m\u001b[32m4035\u001b[39m     \u001b[43msafetensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m   4037\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4039\u001b[39m     torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alber\\Desktop\\projetos\\Generative-IA\\venv-genai\\Lib\\site-packages\\safetensors\\torch.py:488\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    485\u001b[39m         failing.append(names)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[33m        A potential way to correctly save your model is to use `save_model`.\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[33m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[31mRuntimeError\u001b[39m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.lm_head.weight', 'base_model.model.model.decoder.embed_tokens.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Salva somente adapters LoRA\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Treino concluído. Adapters LoRA salvos em:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c63d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Recarrega base + adapters (CPU)\n",
    "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32).to(device)\n",
    "base = PeftModel.from_pretrained(base, OUTPUT_DIR).to(device)\n",
    "base.eval()\n",
    "\n",
    "GEN_CFG = dict(max_new_tokens=128, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "\n",
    "def gerar_sql(pergunta: str, dialeto: str = \"PostgreSQL\") -> str:\n",
    "    prompt = f\"\"\"### Tarefa\n",
    "Escreva a consulta SQL em {dialeto} correspondente à pergunta.\n",
    "\n",
    "### Esquema\n",
    "{SCHEMA.strip()}\n",
    "\n",
    "### Pergunta\n",
    "{pergunta}\n",
    "\n",
    "### Consulta SQL\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = base.generate(**inputs, **GEN_CFG, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste rápido\n",
    "print(gerar_sql(\"Qual é o ticket médio por pedido?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f58ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
