# ğŸ§  MiniGPT - Um Modelo Transformer Causal em PyTorch

Este repositÃ³rio implementa, passo a passo, a arquitetura de um modelo de linguagem (LLM) baseado no **GPT (Generative Pretrained Transformer)**, utilizando **PyTorch puro** e com foco em **clareza educacional**.

Nosso objetivo Ã© construir, a partir do zero, todos os componentes fundamentais de um LLM autoregressivo:

- Embeddings de tokens e posiÃ§Ãµes
- Mecanismo de atenÃ§Ã£o causal (self-attention com mÃ¡scara)
- Empilhamento de blocos Transformer
- MLP (Feedforward)
- NormalizaÃ§Ãµes e conexÃµes residuais
- CabeÃ§a de linguagem para prediÃ§Ã£o de prÃ³ximos tokens
- InferÃªncia e (futuramente) treinamento supervisionado

---

## ğŸ”§ Arquitetura Base

A arquitetura segue o padrÃ£o do GPT original com as seguintes etapas:

```text
Tokens de entrada (B, T)
   â”‚
   â”œâ”€â–º Embedding de tokens â”€â”€â”
   â”œâ”€â–º Embedding de posiÃ§Ãµes â”˜ â†’ soma â†’ x âˆˆ â„^{B Ã— T Ã— d}
   â”‚
   â”œâ”€â–º [ Transformer Block ] Ã— N
   â”‚     â”œâ”€ LayerNorm
   â”‚     â”œâ”€ Self-Attention (multi-head, causal)
   â”‚     â”œâ”€ Residual
   â”‚     â”œâ”€ LayerNorm
   â”‚     â”œâ”€ MLP (GELU)
   â”‚     â””â”€ Residual
   â”‚
   â”œâ”€â–º LayerNorm final
   â””â”€â–º ProjeÃ§Ã£o Linear para vocabulÃ¡rio (â„^{B Ã— T Ã— vocab_size})
