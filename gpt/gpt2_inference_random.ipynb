{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from vocab import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.context_length = 64\n",
    "        self.vocab_size = 200\n",
    "        self.n_layers = 1\n",
    "        self.n_heads = 1\n",
    "        self.n_embd = 16\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embd)     # Word Token Embedding\n",
    "        self.wpe = nn.Embedding(self.context_length, self.n_embd) # Positional Embedding\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_embd, 2 * self.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')  # GELU usada no GPT\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3 * self.n_embd)  # Projeção para Q, K, V\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)      # Projeção final\n",
    "        self.head_dim = self.n_embd // self.n_heads              # Dimensão de cada cabeça\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(self.context_length, self.context_length))\n",
    "                .view(1, 1, self.context_length, self.context_length)\n",
    "        )\n",
    "        \n",
    "        self.final_ln = nn.LayerNorm(self.n_embd)  # LayerNorm final\n",
    "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size)  # Mapeia para o vocabulário\n",
    "\n",
    "\n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "    def mlp(self, x):\n",
    "        # x: (T, C)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def layer_norm(self, x):\n",
    "        # x: (T, C)\n",
    "        return self.ln(x)\n",
    "    \n",
    "    def self_attention(self, x):\n",
    "        # x: (T, C) → sequência, embedding\n",
    "        T, C = x.size()\n",
    "\n",
    "        # Projeta para Q, K, V\n",
    "        qkv = self.qkv_proj(x)  # (T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)  # Cada um (T, C)\n",
    "\n",
    "        # Separa em múltiplas heads\n",
    "        #q = q.view(T, self.n_heads, self.head_dim).transpose(0, 1)  # (nh, T, hd)\n",
    "        #k = k.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        #v = v.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # Produto escalar entre Q e Kᵀ\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (nh, T, T)\n",
    "\n",
    "        # Aplicar máscara causal\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Normaliza com Softmax\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # Atenção aplicada sobre V\n",
    "        y = att @ v  # (nh, T, hd)\n",
    "\n",
    "        # Junta as heads\n",
    "        #y = y.transpose(0, 1).contiguous().view(T, C)  # (T, C)\n",
    "\n",
    "        # Projeção final\n",
    "        y = self.out_proj(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def tokens_idx(self, tokens_chosen):\n",
    "        self.tokens_vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.tokens_vocab.items()}\n",
    "\n",
    "        # Pegar os índices correspondentes\n",
    "        indices = [self.tokens_vocab[token] for token in tokens_chosen]\n",
    "\n",
    "        # Converter para tensor, se quiser passar ao modelo\n",
    "        self.indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        #print(\"Tokens escolhidos:\", tokens_escolhidos)\n",
    "        #print(\"Índices encontrados:\", indices)\n",
    "        #print(\"Tensor de índices:\", indices_tensor)\n",
    "        #return indices_tensor\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Executa a passagem para frente (forward pass) do modelo.\n",
    "        Usa os índices de tokens armazenados em self.indices_tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # === Entrada: índices dos tokens ===\n",
    "        idx = self.indices_tensor  # Tensor de índices (T,)\n",
    "        T = idx.size(0)             # Número de tokens\n",
    "        print(f\"Tamanho da sequência (T): {T}\")\n",
    "        print(f\"Shape de idx: {idx.shape}\")  # (T,)\n",
    "\n",
    "        # === Embedding dos tokens ===\n",
    "        tok_emb = self.wte(idx)  # Embedding dos tokens (T, n_embd)\n",
    "        print(\"\\nEmbedding dos Tokens (tok_emb):\")\n",
    "        print(f\"Shape: {tok_emb.shape}\")  # (T, n_embd)\n",
    "        #print(tok_emb)\n",
    "\n",
    "        # === Embedding das posições ===\n",
    "        positions = torch.arange(T, dtype=torch.long, device=idx.device)  # (T,)\n",
    "        pos_emb = self.wpe(positions)  # Embedding das posições (T, n_embd)\n",
    "        print(\"\\nEmbedding das Posições (pos_emb):\")\n",
    "        print(f\"Shape: {pos_emb.shape}\")  # (T, n_embd)\n",
    "        #print(pos_emb)\n",
    "\n",
    "        # === Soma dos embeddings ===\n",
    "        x = tok_emb + pos_emb  # Combinação token + posição (T, n_embd)\n",
    "        print(\"\\nSoma dos Embeddings (x = tok_emb + pos_emb):\")\n",
    "        print(f\"Shape: {x.shape}\")  # (T, n_embd)\n",
    "        #print(x)\n",
    "\n",
    "        # === Normalização antes da atenção ===\n",
    "        x_norm = self.layer_norm(x)  # (T, n_embd)\n",
    "        print(\"\\nLayerNorm aplicado na soma (x_norm):\")\n",
    "        print(f\"Shape: {x_norm.shape}\")  # (T, n_embd)\n",
    "        #print(x_norm)\n",
    "\n",
    "        # === Atenção (Self-Attention) ===\n",
    "        attn_out = self.self_attention(x_norm)  # (T, n_embd)\n",
    "        print(\"\\nSaída da Self-Attention (attn_out):\")\n",
    "        print(f\"Shape: {attn_out.shape}\")  # (T, n_embd)\n",
    "        #print(attn_out)\n",
    "\n",
    "        # === Soma residual (após atenção) ===\n",
    "        x = x + attn_out  # (T, n_embd)\n",
    "        print(\"\\nSoma Residual após Atenção (x):\")\n",
    "        print(f\"Shape: {x.shape}\")  # (T, n_embd)\n",
    "        #print(x)\n",
    "\n",
    "        # === Segundo LayerNorm (antes do MLP) ===\n",
    "        x_norm = self.layer_norm(x)  # (T, n_embd)\n",
    "        print(\"\\nSegundo LayerNorm antes do MLP (x_norm):\")\n",
    "        print(f\"Shape: {x_norm.shape}\")  # (T, n_embd)\n",
    "        #print(x_norm)\n",
    "\n",
    "        # === Passagem pelo MLP ===\n",
    "        mlp_out = self.mlp(x_norm)  # (T, n_embd)\n",
    "        print(\"\\nSaída do MLP (mlp_out):\")\n",
    "        print(f\"Shape: {mlp_out.shape}\")  # (T, n_embd)\n",
    "        #print(mlp_out)\n",
    "\n",
    "        # === Soma Residual após o MLP ===\n",
    "        x = x + mlp_out  # (T, n_embd)\n",
    "        print(\"\\nSoma Residual após o MLP (x):\")\n",
    "        print(f\"Shape: {x.shape}\")  # (T, n_embd)\n",
    "        #print(x)\n",
    "\n",
    "        return x[0,0,-1,:]\n",
    "    \n",
    "    def predict_logits(self, x):\n",
    "        \"\"\"\n",
    "        Recebe a saída do forward (T, n_embd),\n",
    "        aplica LayerNorm final e gera logits para o vocabulário (T, vocab_size).\n",
    "        \"\"\"\n",
    "        x = self.final_ln(x)  # Normaliza novamente\n",
    "        logits = self.lm_head(x)  # Projeta para vocab_size\n",
    "        print(\"\\nLogits (após LayerNorm final):\")\n",
    "        print(f\"Shape: {logits.shape}\")  # (T, vocab_size)\n",
    "        #print(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "45d1f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61310f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_escolhidos = [\"o\", \"gato\", \"pequeno\"]\n",
    "model.tokens_idx(tokens_escolhidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ea2ad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da sequência (T): 3\n",
      "Shape de idx: torch.Size([3])\n",
      "\n",
      "Embedding dos Tokens (tok_emb):\n",
      "Shape: torch.Size([3, 16])\n",
      "\n",
      "Embedding das Posições (pos_emb):\n",
      "Shape: torch.Size([3, 16])\n",
      "\n",
      "Soma dos Embeddings (x = tok_emb + pos_emb):\n",
      "Shape: torch.Size([3, 16])\n",
      "\n",
      "LayerNorm aplicado na soma (x_norm):\n",
      "Shape: torch.Size([3, 16])\n",
      "\n",
      "Saída da Self-Attention (attn_out):\n",
      "Shape: torch.Size([1, 1, 3, 16])\n",
      "\n",
      "Soma Residual após Atenção (x):\n",
      "Shape: torch.Size([1, 1, 3, 16])\n",
      "\n",
      "Segundo LayerNorm antes do MLP (x_norm):\n",
      "Shape: torch.Size([1, 1, 3, 16])\n",
      "\n",
      "Saída do MLP (mlp_out):\n",
      "Shape: torch.Size([1, 1, 3, 16])\n",
      "\n",
      "Soma Residual após o MLP (x):\n",
      "Shape: torch.Size([1, 1, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "res = model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89f17da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logits (após LayerNorm final):\n",
      "Shape: torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "logits = model.predict_logits(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8446872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0104, 0.0085, 0.0074, 0.0070, 0.0028, 0.0048, 0.0049, 0.0068, 0.0068,\n",
       "        0.0040, 0.0113, 0.0071, 0.0034, 0.0067, 0.0078, 0.0019, 0.0027, 0.0070,\n",
       "        0.0071, 0.0033, 0.0024, 0.0053, 0.0029, 0.0052, 0.0045, 0.0027, 0.0028,\n",
       "        0.0033, 0.0053, 0.0048, 0.0071, 0.0037, 0.0034, 0.0078, 0.0031, 0.0027,\n",
       "        0.0048, 0.0086, 0.0057, 0.0088, 0.0054, 0.0044, 0.0038, 0.0014, 0.0013,\n",
       "        0.0052, 0.0042, 0.0033, 0.0047, 0.0053, 0.0025, 0.0036, 0.0086, 0.0036,\n",
       "        0.0014, 0.0016, 0.0025, 0.0013, 0.0043, 0.0061, 0.0036, 0.0014, 0.0045,\n",
       "        0.0018, 0.0024, 0.0109, 0.0063, 0.0133, 0.0026, 0.0039, 0.0049, 0.0063,\n",
       "        0.0047, 0.0063, 0.0084, 0.0063, 0.0072, 0.0029, 0.0012, 0.0030, 0.0086,\n",
       "        0.0069, 0.0043, 0.0019, 0.0076, 0.0058, 0.0130, 0.0035, 0.0069, 0.0020,\n",
       "        0.0028, 0.0024, 0.0027, 0.0111, 0.0039, 0.0052, 0.0044, 0.0043, 0.0024,\n",
       "        0.0044, 0.0086, 0.0036, 0.0036, 0.0042, 0.0024, 0.0053, 0.0017, 0.0037,\n",
       "        0.0029, 0.0025, 0.0047, 0.0047, 0.0020, 0.0074, 0.0062, 0.0019, 0.0015,\n",
       "        0.0083, 0.0061, 0.0031, 0.0025, 0.0059, 0.0020, 0.0053, 0.0032, 0.0186,\n",
       "        0.0038, 0.0095, 0.0011, 0.0027, 0.0053, 0.0014, 0.0029, 0.0045, 0.0043,\n",
       "        0.0067, 0.0019, 0.0070, 0.0057, 0.0037, 0.0096, 0.0029, 0.0040, 0.0111,\n",
       "        0.0082, 0.0021, 0.0046, 0.0041, 0.0068, 0.0067, 0.0065, 0.0146, 0.0009,\n",
       "        0.0076, 0.0052, 0.0045, 0.0070, 0.0080, 0.0026, 0.0045, 0.0026, 0.0058,\n",
       "        0.0045, 0.0020, 0.0023, 0.0071, 0.0064, 0.0046, 0.0081, 0.0018, 0.0102,\n",
       "        0.0018, 0.0052, 0.0030, 0.0025, 0.0092, 0.0021, 0.0089, 0.0073, 0.0047,\n",
       "        0.0088, 0.0034, 0.0022, 0.0033, 0.0047, 0.0062, 0.0012, 0.0077, 0.0041,\n",
       "        0.0018, 0.0223, 0.0055, 0.0074, 0.0068, 0.0015, 0.0015, 0.0011, 0.0042,\n",
       "        0.0022, 0.0031], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(logits, dim=-1)  # (T, vocab_size)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80e0f2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "next_token_idx = int(next_token_idx[0])\n",
    "next_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ccb7aa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'porque'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.idx_to_token[next_token_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d72df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
