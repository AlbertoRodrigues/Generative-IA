{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.context_length = 64\n",
    "        self.vocab_size = 1000\n",
    "        self.n_layers = 1\n",
    "        self.n_heads = 1\n",
    "        self.n_embd = 64\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_embd, 2 * self.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')  # GELU usada no GPT\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3 * self.n_embd)  # Projeção para Q, K, V\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)      # Projeção final\n",
    "        self.head_dim = self.n_embd // self.n_heads              # Dimensão de cada cabeça\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(self.context_length, self.context_length))\n",
    "                .view(1, 1, self.context_length, self.context_length)\n",
    "        )\n",
    "        \n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "    def mlp_forward(self, x):\n",
    "        # x: (T, C)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def layernorm_forward(self, x):\n",
    "        # x: (T, C)\n",
    "        return self.ln(x)\n",
    "    \n",
    "    def self_attention(self, x):\n",
    "        # x: (T, C) → sequência, embedding\n",
    "        T, C = x.size()\n",
    "\n",
    "        # Projeta para Q, K, V\n",
    "        qkv = self.qkv_proj(x)  # (T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)  # Cada um (T, C)\n",
    "\n",
    "        # Separa em múltiplas heads\n",
    "        #q = q.view(T, self.n_heads, self.head_dim).transpose(0, 1)  # (nh, T, hd)\n",
    "        #k = k.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        #v = v.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # Produto escalar entre Q e Kᵀ\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (nh, T, T)\n",
    "\n",
    "        # Aplicar máscara causal\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Normaliza com Softmax\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # Atenção aplicada sobre V\n",
    "        y = att @ v  # (nh, T, hd)\n",
    "\n",
    "        # Junta as heads\n",
    "        y = y.transpose(0, 1).contiguous().view(T, C)  # (T, C)\n",
    "\n",
    "        # Projeção final\n",
    "        y = self.out_proj(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072fca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
