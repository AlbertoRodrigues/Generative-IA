{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from vocab import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vocab import tokens\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_tokens     = 12\n",
    "        self.context_length = 16\n",
    "        self.vocab_size     = 209\n",
    "        self.n_layers       = 2      # agora duas camadas\n",
    "        self.n_heads        = 4      # agora quatro cabeças\n",
    "        self.n_embd         = 16\n",
    "\n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embd)\n",
    "        self.wpe = nn.Embedding(self.context_length, self.n_embd)\n",
    "\n",
    "        # MLP\n",
    "        self.fc1 = nn.Linear(self.n_embd, 2 * self.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.fc2 = nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "\n",
    "        # Normalização\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "        self.ln.weight.requires_grad = False\n",
    "        self.ln.bias.requires_grad = False\n",
    "\n",
    "        # Atenção\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.head_dim = self.n_embd // self.n_heads\n",
    "\n",
    "        # Máscara causal\n",
    "        self.mask = torch.tril(torch.ones(self.context_length,\n",
    "                                          self.context_length)\n",
    "                               ).view(self.context_length,\n",
    "                                      self.context_length)\n",
    "\n",
    "        # Final head\n",
    "        self.final_ln = nn.LayerNorm(self.n_embd)\n",
    "        self.lm_head   = nn.Linear(self.n_embd, self.vocab_size)\n",
    "\n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "    def init_tokens(self, tokens_list):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.tokens_vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.tokens_vocab.items()}\n",
    "\n",
    "    def mlp(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def layer_norm(self, x):\n",
    "        return self.ln(x)\n",
    "\n",
    "    def self_attention(self, x):\n",
    "        # x: (T, C)\n",
    "        T, C = x.size()\n",
    "        qkv = self.qkv_proj(x)               # (T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)        # cada um (T, C)\n",
    "\n",
    "        # projeção multi-head\n",
    "        q = q.view(T, self.n_heads, self.head_dim).transpose(0,1)  # (nh, T, hd)\n",
    "        k = k.view(T, self.n_heads, self.head_dim).transpose(0,1)\n",
    "        v = v.view(T, self.n_heads, self.head_dim).transpose(0,1)\n",
    "\n",
    "        # produto escalar e escala\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim)   # (nh, T, T)\n",
    "\n",
    "        # máscara causal\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # softmax\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # aplicação sobre v\n",
    "        y = att @ v  # (nh, T, hd)\n",
    "\n",
    "        # concatena heads\n",
    "        y = y.transpose(0,1).contiguous().view(T, C)  # (T, C)\n",
    "\n",
    "        # projeção final\n",
    "        return self.out_proj(y)\n",
    "\n",
    "    def tokens_idx(self, tokens_chosen):\n",
    "        # Pegar os índices correspondentes\n",
    "        indices = [self.tokens_vocab[token] for token in tokens_chosen]\n",
    "\n",
    "        # Converter para tensor, se quiser passar ao modelo\n",
    "        self.indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def forward(self):\n",
    "        # 1) lookup embeddings\n",
    "        idx = self.indices_tensor         # (T,)\n",
    "        T   = idx.size(0)\n",
    "        tok_emb = self.wte(idx)           # (T, n_embd)\n",
    "        pos_emb = self.wpe(torch.arange(T, device=idx.device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb             # (T, n_embd)\n",
    "\n",
    "        # 2) aplica N camadas de (LN → Atenção → Residual → LN → MLP → Residual)\n",
    "        for _ in range(self.n_layers):\n",
    "            # pré-atenção\n",
    "            x_norm = self.layer_norm(x)\n",
    "            attn_out = self.self_attention(x_norm)\n",
    "            x = x + attn_out\n",
    "\n",
    "            # pré-MLP\n",
    "            x_norm = self.layer_norm(x)\n",
    "            mlp_out = self.mlp(x_norm)\n",
    "            x = x + mlp_out\n",
    "\n",
    "        return x[-1]  # vetor do último token (n_embd,)\n",
    "\n",
    "    def predict_logits(self, x):\n",
    "        x = self.final_ln(x)  # (n_embd,)\n",
    "        return self.lm_head(x)  # (vocab_size,)\n",
    "\n",
    "    def predict_next_token(self):\n",
    "        self.tokens_idx(self.tokens_list)\n",
    "        vec = self.forward()               # (n_embd,)\n",
    "        logits = self.predict_logits(vec)  # (vocab_size,)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx  = torch.multinomial(probs, num_samples=1).item()\n",
    "        return self.idx_to_token[idx]\n",
    "\n",
    "    def predict_all_sentence(self):\n",
    "        # guarda cópia do estado original\n",
    "        original = list(self.tokens_list)\n",
    "\n",
    "        for step in range(self.max_tokens):\n",
    "            nt = self.predict_next_token()\n",
    "            self.tokens_list.append(nt)\n",
    "            print(f\"Passo {step+1}: {' '.join(self.tokens_list)}\")\n",
    "\n",
    "        # recupera o estado original\n",
    "        self.tokens_list = original\n",
    "        return list(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4204617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [\"o\", \"gato\", \"pequeno\"]\n",
    "model = GPT()\n",
    "model.init_tokens(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0e9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome: wte.weight\n",
      "Shape: torch.Size([209, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[-0.1242, -0.1960,  0.0392,  ...,  0.3843,  2.3576,  0.7143],\n",
      "        [ 0.9828,  2.3151, -0.1122,  ...,  0.9089, -0.8447, -0.6665],\n",
      "        [ 0.4429,  0.1570,  1.1231,  ...,  0.7481,  1.2063, -1.7450],\n",
      "        ...,\n",
      "        [ 0.1875, -0.1669, -0.4234,  ...,  0.3493, -1.2399, -2.1087],\n",
      "        [ 0.7212, -0.1140, -0.9392,  ...,  0.1211, -1.4641,  0.4216],\n",
      "        [ 0.4343,  0.4413, -0.4894,  ...,  0.9323, -0.7992, -0.1888]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: wpe.weight\n",
      "Shape: torch.Size([16, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 1.1967, -0.2463, -0.8689,  0.1212, -2.2817,  1.1339,  0.4178,  1.9397,\n",
      "          1.5714,  1.5718, -0.7243, -0.1788,  0.3221, -0.1210,  0.8099,  0.3727],\n",
      "        [-0.6813, -1.4569, -0.5004, -1.0607, -1.5872, -0.2846, -0.5316, -0.2955,\n",
      "         -0.1098,  1.5127,  1.5372, -1.2987,  0.9330, -0.6001, -1.4782,  0.1846],\n",
      "        [-1.1192, -0.2405,  0.4051, -0.1834, -0.3536,  0.7367, -0.8377,  0.5855,\n",
      "          0.4578, -0.8669, -0.7841, -0.5659,  0.8267, -0.4315,  0.5164,  0.2536],\n",
      "        [-1.0245,  2.2203,  0.9551, -0.6958, -1.2601,  1.6626, -0.6594,  0.5487,\n",
      "          0.6414,  0.4497,  0.3032,  0.1536, -0.6509, -0.2143, -0.9201, -0.5876],\n",
      "        [-0.0693,  0.1357, -0.2744, -0.7727, -1.1511, -0.2361,  0.1400, -0.4536,\n",
      "         -0.7846,  0.5583,  0.1166,  1.0813,  0.2828, -2.0365,  2.0701,  0.0687],\n",
      "        [ 0.2743, -1.2051,  0.2315, -0.5077, -0.2074, -1.0461, -0.3207, -0.4881,\n",
      "         -1.1761, -1.5272,  2.4045,  0.1252, -0.4917,  0.2552, -1.2388, -0.2152],\n",
      "        [-0.4918,  0.1141, -2.0517,  0.6323, -0.2032,  0.4462,  0.8741,  0.7836,\n",
      "          2.9671, -0.3592,  2.2812, -0.7021, -1.3427, -0.4093,  0.7781, -0.9899],\n",
      "        [-0.4706,  1.9014, -1.2285,  0.9651,  0.8665, -0.0688, -1.6260, -1.0190,\n",
      "          1.6996, -0.1770,  1.4654,  1.7737,  0.3902, -0.3368, -0.3422, -1.1048],\n",
      "        [ 0.1834,  1.0939,  1.3374, -0.9578, -0.5121, -0.8066, -0.6996, -1.5661,\n",
      "          0.0825, -0.4181, -0.4754, -1.1051, -2.6265,  0.3006,  0.4855, -0.0991],\n",
      "        [ 0.3125, -1.8321, -1.6104,  0.5228, -0.6373, -1.4702,  0.2668, -0.5416,\n",
      "         -1.2985,  1.1324,  1.6170,  1.6111, -0.6729, -0.1365,  1.9069,  0.2672],\n",
      "        [-1.0698,  1.1754, -0.8892,  0.7199,  0.4026,  0.7766, -0.0348, -0.0625,\n",
      "          0.2197,  1.0990,  0.6239, -1.7858, -0.4186,  1.0575,  0.3109, -0.4418],\n",
      "        [-0.5847,  1.2067,  1.7361,  0.5706, -0.5037,  0.7411,  0.9836,  1.6347,\n",
      "          0.5095,  2.1583, -0.0360,  0.9186,  1.1970,  1.3480, -1.9305,  1.0961],\n",
      "        [ 0.0662,  2.1282, -1.1251,  0.5063, -1.3809,  0.5759,  0.7070, -1.6108,\n",
      "         -0.2197, -1.6814,  0.1762,  0.3809, -0.5404,  0.3266,  0.6253, -0.4610],\n",
      "        [ 0.9839, -0.4882, -0.5427,  0.3949, -0.9902, -0.5213, -1.0843,  0.7539,\n",
      "          0.7837, -0.6290,  0.5557,  1.6345,  0.8675, -1.1256, -0.6376, -1.8818],\n",
      "        [ 1.2001,  1.2912, -1.1652,  0.0037, -0.3720,  1.1961,  0.9525,  0.6604,\n",
      "          0.3237,  1.2824, -0.8756,  1.4862, -0.4121, -2.0793, -0.5470, -0.8974],\n",
      "        [-0.4107, -1.2751,  1.0030, -0.3549, -1.3799, -0.9427,  0.1756, -0.1696,\n",
      "         -0.1613, -1.0229, -0.9658, -0.2028, -1.7448, -0.3018, -1.2027, -0.0676]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc1.weight\n",
      "Shape: torch.Size([32, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 4.3439e-02,  3.9927e-02, -6.2495e-02,  2.1242e-03, -1.3945e-01,\n",
      "         -1.7443e-01,  1.9899e-01,  1.5267e-03, -2.9686e-02, -5.7437e-02,\n",
      "          2.1747e-01, -2.4802e-01, -8.7179e-02,  2.0625e-01, -9.0719e-02,\n",
      "         -2.9549e-02],\n",
      "        [-1.7435e-01,  7.9463e-02, -9.3082e-02,  8.1429e-02, -4.7490e-02,\n",
      "          6.4068e-02,  6.9893e-02, -1.6012e-01, -3.6368e-02, -1.0328e-01,\n",
      "          2.0712e-01, -2.0009e-01, -6.6865e-02, -5.0059e-02, -4.8315e-02,\n",
      "         -2.0481e-01],\n",
      "        [-3.6968e-02, -8.8040e-02, -3.0112e-02,  1.1408e-01,  2.6010e-02,\n",
      "         -2.3104e-01, -2.0494e-01,  7.9436e-02,  1.6152e-01,  1.2716e-01,\n",
      "         -8.2045e-02,  3.1380e-02, -1.9064e-01, -1.9864e-01, -1.9452e-01,\n",
      "         -1.3334e-02],\n",
      "        [-8.8424e-02,  8.7069e-02,  1.4134e-02, -1.7308e-01,  7.6686e-02,\n",
      "         -1.1350e-01, -5.0487e-02,  6.1448e-02, -4.2188e-03,  5.9977e-02,\n",
      "          1.8339e-01, -2.2188e-01, -1.7641e-01,  1.2178e-01, -2.3228e-01,\n",
      "          5.8098e-02],\n",
      "        [-1.6094e-02, -1.4502e-01, -1.3103e-01, -1.9686e-02, -2.0604e-01,\n",
      "          4.0874e-02, -1.7140e-01,  6.6920e-02,  2.7402e-03,  2.3852e-01,\n",
      "          1.1927e-01, -7.9204e-02, -1.2587e-01,  1.7405e-01,  3.4133e-02,\n",
      "         -2.0973e-01],\n",
      "        [ 2.4826e-01, -2.3240e-01, -1.0984e-01, -1.8886e-01,  2.2097e-01,\n",
      "         -1.3908e-01, -2.0536e-01,  2.3473e-01, -2.3986e-01, -9.3965e-02,\n",
      "         -2.1105e-01,  3.4393e-02,  1.3288e-01,  6.3541e-02,  2.0155e-01,\n",
      "          1.4685e-01],\n",
      "        [-1.5263e-01,  1.1812e-01,  5.6473e-02,  2.1389e-01, -7.5270e-02,\n",
      "         -1.7545e-01, -3.1491e-02, -1.3290e-01, -1.5646e-01, -2.0180e-01,\n",
      "         -1.1201e-01,  8.5505e-02, -1.6175e-01, -8.8835e-02,  3.7557e-02,\n",
      "          6.4512e-03],\n",
      "        [ 1.4182e-01, -1.7691e-01,  7.7455e-02,  2.2712e-01, -1.5741e-01,\n",
      "         -2.4965e-01, -1.6953e-01, -1.1707e-01, -4.9413e-02, -5.4931e-02,\n",
      "         -3.2845e-02,  4.3760e-02, -2.0723e-01,  1.3844e-01,  2.4807e-01,\n",
      "         -1.8092e-01],\n",
      "        [-1.6495e-01,  2.3316e-01, -1.2729e-02,  5.0582e-02, -1.9023e-01,\n",
      "          1.1578e-01,  3.5085e-02, -1.5749e-01,  1.4439e-01, -1.1087e-01,\n",
      "          1.9754e-01, -4.2748e-02, -1.6075e-01, -2.3414e-01, -4.9217e-02,\n",
      "         -3.4741e-02],\n",
      "        [ 7.4762e-02,  2.3322e-01, -1.4764e-01, -7.8680e-02,  2.3860e-01,\n",
      "          1.9442e-01,  2.4090e-01, -9.0259e-02,  2.5573e-02,  5.5022e-02,\n",
      "         -1.5284e-01, -2.9021e-02,  1.8442e-01, -1.5254e-01, -4.4147e-02,\n",
      "         -6.9412e-02],\n",
      "        [-5.7003e-02,  1.6615e-01,  4.5647e-03, -2.4222e-01,  3.4024e-02,\n",
      "         -2.4103e-01,  1.4035e-01, -2.2749e-01, -2.3524e-01,  1.2557e-02,\n",
      "         -2.2090e-01,  2.2417e-01,  4.9729e-02, -1.7294e-02,  1.2775e-01,\n",
      "         -7.5504e-02],\n",
      "        [-1.1951e-01,  1.7620e-01, -2.4578e-01, -7.5807e-02, -1.8005e-01,\n",
      "          2.4232e-01, -6.6719e-02,  1.8947e-01, -1.3689e-01, -2.0579e-01,\n",
      "         -1.7134e-01, -1.0390e-01, -2.2444e-01, -2.4430e-01, -1.7472e-01,\n",
      "         -2.2723e-02],\n",
      "        [-2.4489e-01,  9.5780e-03, -5.1633e-02,  1.8082e-01, -8.2270e-02,\n",
      "         -8.0861e-02,  5.9664e-02, -1.3421e-01, -6.0789e-02, -1.3695e-01,\n",
      "          1.1928e-01,  7.1903e-02,  9.1422e-02, -7.9270e-02, -1.8296e-01,\n",
      "          1.3612e-01],\n",
      "        [-2.3932e-02, -4.3642e-02, -1.6994e-01,  1.7489e-01, -7.6417e-02,\n",
      "          2.3541e-01,  5.4645e-02,  1.7883e-01, -1.1850e-01,  2.2916e-01,\n",
      "         -1.5475e-01,  1.6441e-01, -1.3827e-01,  2.4140e-01,  1.3840e-01,\n",
      "          1.7017e-01],\n",
      "        [-1.6297e-01,  9.1675e-02, -2.4170e-01, -2.3630e-01, -1.5547e-01,\n",
      "          1.1678e-01, -3.6727e-02,  1.2393e-01,  2.1145e-01,  2.0172e-01,\n",
      "          2.1214e-01,  9.4743e-02,  7.6154e-03, -2.0970e-01,  1.5657e-01,\n",
      "         -1.6421e-01],\n",
      "        [-1.3396e-01, -1.5028e-01,  2.0279e-01,  2.0695e-01, -2.2243e-01,\n",
      "          1.7607e-01,  3.7099e-03, -1.1345e-01,  1.9292e-01,  1.8345e-01,\n",
      "          6.8724e-02,  1.0054e-01, -1.1412e-01,  2.4067e-01, -1.7653e-01,\n",
      "          2.4703e-01],\n",
      "        [-1.9084e-01,  2.0553e-01,  3.5446e-02,  1.3733e-01, -2.2537e-01,\n",
      "         -1.7045e-01,  2.3295e-01, -2.1249e-01, -2.2174e-01, -9.3390e-02,\n",
      "          1.0591e-01, -1.8463e-01,  1.9002e-01,  9.5840e-02, -2.4403e-01,\n",
      "          2.6382e-03],\n",
      "        [ 1.4663e-01, -6.2029e-02, -2.2339e-01,  1.3741e-01,  2.3113e-01,\n",
      "          1.5865e-01, -1.6183e-01, -1.6249e-01, -3.2029e-02,  1.7880e-02,\n",
      "          1.1303e-02, -2.0352e-01,  6.5307e-02, -1.5472e-01,  2.2814e-01,\n",
      "         -2.9368e-02],\n",
      "        [-1.5859e-01, -1.4381e-01, -2.4735e-01, -1.3674e-01, -1.6777e-01,\n",
      "          5.8944e-02,  1.4722e-01, -1.8099e-01, -6.4094e-02,  1.9103e-01,\n",
      "          8.0721e-02,  1.2294e-01,  1.2821e-01,  2.0412e-01,  1.3720e-01,\n",
      "          6.1619e-02],\n",
      "        [ 6.7666e-02,  1.2947e-01, -1.3333e-01, -2.2638e-01,  1.1966e-01,\n",
      "          2.0500e-01,  2.5524e-02, -6.3762e-02, -6.5068e-03, -1.2027e-01,\n",
      "         -1.6650e-01,  6.7508e-02, -2.3780e-01, -1.4232e-01,  2.8277e-02,\n",
      "         -1.8159e-01],\n",
      "        [-6.0965e-02, -8.4195e-02,  1.7594e-01,  4.9206e-02,  3.2716e-02,\n",
      "          1.0506e-01, -4.1854e-02, -2.2129e-01,  8.2334e-02,  2.2054e-01,\n",
      "          6.7772e-02,  2.0268e-02,  1.6402e-02,  7.0678e-03, -1.9384e-01,\n",
      "         -6.1751e-02],\n",
      "        [ 1.5301e-01,  1.8741e-01,  1.7545e-01,  1.0097e-01,  7.6845e-02,\n",
      "         -1.3798e-01, -1.5122e-01,  8.1644e-02, -1.5909e-01,  1.1062e-01,\n",
      "         -1.8816e-01, -5.3528e-02,  1.5087e-01, -1.8317e-01,  1.3310e-02,\n",
      "          6.7393e-02],\n",
      "        [-2.0868e-01, -1.3263e-01, -1.1856e-01,  9.1166e-02, -1.7518e-01,\n",
      "         -6.3726e-03, -8.3009e-02,  2.1639e-01,  9.8073e-02, -6.6075e-02,\n",
      "         -2.5345e-02,  1.8904e-01, -2.1779e-01, -2.4387e-01, -8.0423e-02,\n",
      "         -2.2259e-01],\n",
      "        [ 2.4016e-02, -1.6482e-02,  2.6196e-05, -1.4096e-02, -3.7464e-04,\n",
      "         -1.1648e-02,  2.3530e-01,  1.8754e-01,  2.7474e-02, -1.9973e-01,\n",
      "         -8.0011e-02, -1.2158e-01, -1.4240e-01, -1.0089e-01,  1.5653e-01,\n",
      "         -2.4462e-01],\n",
      "        [ 2.3338e-01,  1.5632e-01, -2.3556e-01, -8.9126e-02, -5.7026e-02,\n",
      "         -1.8802e-01,  4.5316e-02, -1.7970e-01, -1.8642e-03,  7.4390e-02,\n",
      "         -1.8479e-01,  2.2148e-01, -2.3167e-02, -2.3280e-02,  1.0111e-02,\n",
      "          2.1924e-01],\n",
      "        [-1.1243e-01, -7.8747e-02, -1.9482e-01,  1.0434e-01, -6.0232e-02,\n",
      "         -1.0891e-01,  1.3002e-01,  9.6854e-02, -1.5635e-01,  1.1727e-01,\n",
      "          7.5786e-03,  1.9552e-01,  2.0405e-01,  2.1986e-01, -1.1684e-03,\n",
      "         -6.7431e-02],\n",
      "        [-1.9120e-01, -3.8820e-02, -1.8545e-01, -1.6442e-01,  1.5853e-01,\n",
      "         -1.2179e-02,  6.4759e-02, -2.0686e-02,  1.5231e-01,  4.2593e-02,\n",
      "          7.7737e-02,  1.5201e-01,  1.5498e-01, -1.3342e-01, -1.3226e-01,\n",
      "         -1.8308e-01],\n",
      "        [ 1.1673e-01,  7.7653e-02,  1.2380e-01,  2.0403e-01,  4.7792e-02,\n",
      "          1.8840e-01, -2.1120e-01, -1.8404e-01,  1.7192e-01, -1.3013e-01,\n",
      "          7.2585e-02, -1.1180e-01,  1.7483e-01, -2.4516e-01,  1.6926e-02,\n",
      "         -1.9827e-01],\n",
      "        [ 1.0823e-01,  2.6367e-03, -2.1600e-01, -9.6408e-02, -2.0119e-02,\n",
      "          2.2776e-01, -1.9904e-01,  9.7901e-02, -1.6140e-01,  1.0342e-01,\n",
      "         -2.0347e-01, -3.7171e-02,  2.1965e-01, -5.1774e-02, -2.3613e-01,\n",
      "          8.0202e-02],\n",
      "        [ 1.4431e-01, -6.3394e-02, -2.4192e-01, -9.9050e-02,  1.6740e-01,\n",
      "          1.7609e-01,  2.0338e-01,  1.4386e-01,  1.9026e-01, -1.1240e-01,\n",
      "          4.7409e-02,  2.5195e-02,  4.0243e-02, -8.5384e-02,  2.3947e-01,\n",
      "         -1.7879e-01],\n",
      "        [ 1.5558e-02, -1.0994e-01, -9.0812e-02, -7.6459e-03, -8.5749e-02,\n",
      "         -8.0233e-03,  8.8430e-03,  1.6135e-01,  1.6349e-02,  1.4339e-02,\n",
      "          2.1175e-01, -1.0896e-01,  1.1219e-01, -1.5091e-01,  1.7321e-01,\n",
      "          1.8802e-01],\n",
      "        [-1.2722e-01, -1.5519e-01,  6.4256e-02, -2.0944e-01, -1.7461e-01,\n",
      "          2.1563e-02, -5.0236e-02,  1.5010e-01,  2.4868e-01, -1.1974e-01,\n",
      "          1.6687e-01, -8.3438e-02,  7.1084e-02, -7.4922e-02,  2.2401e-01,\n",
      "          1.8763e-01]], requires_grad=True)\n",
      "\n",
      "Nome: fc1.bias\n",
      "Shape: torch.Size([32])\n",
      "Valores: Parameter containing:\n",
      "tensor([ 0.1325,  0.1629,  0.0937,  0.1088, -0.0260,  0.2232, -0.1488,  0.0718,\n",
      "         0.0329,  0.1614,  0.1774,  0.0862,  0.0800, -0.0111, -0.1938, -0.0992,\n",
      "         0.0851, -0.2437, -0.1465, -0.2326, -0.1661, -0.2123, -0.1279, -0.1597,\n",
      "        -0.1937,  0.0424,  0.0739, -0.1960,  0.1282, -0.0494, -0.2191, -0.2376],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc2.weight\n",
      "Shape: torch.Size([16, 32])\n",
      "Valores: Parameter containing:\n",
      "tensor([[-0.1505, -0.1173,  0.0423, -0.0601,  0.1181, -0.0650, -0.0840,  0.1484,\n",
      "          0.0941, -0.0337,  0.0813,  0.1190, -0.1324, -0.0047, -0.0569, -0.1571,\n",
      "         -0.1055, -0.0240, -0.1274,  0.0550,  0.1426, -0.0652,  0.1063,  0.1611,\n",
      "          0.1426, -0.0582, -0.0086,  0.0873, -0.0157, -0.1340, -0.0354, -0.0328],\n",
      "        [-0.0476,  0.1519, -0.0823, -0.1416,  0.0270, -0.1647,  0.1413,  0.0930,\n",
      "         -0.0688,  0.0645, -0.0071,  0.0492,  0.1198,  0.1691,  0.0255, -0.1127,\n",
      "         -0.1394,  0.0898, -0.1045, -0.0870,  0.0378, -0.1621, -0.0582,  0.1041,\n",
      "          0.0436,  0.1527, -0.0486, -0.1562,  0.1173, -0.1612, -0.0639, -0.1561],\n",
      "        [ 0.0743, -0.1680,  0.1515,  0.1121, -0.0926, -0.1396, -0.0593, -0.0483,\n",
      "          0.1049, -0.0974,  0.0287,  0.0298, -0.1515, -0.0907,  0.1331, -0.0846,\n",
      "         -0.0990,  0.1690,  0.0415, -0.1058,  0.0720, -0.0406, -0.1483, -0.0462,\n",
      "          0.1581,  0.0972, -0.1089, -0.1721,  0.0288,  0.1066, -0.0170,  0.0748],\n",
      "        [ 0.0843,  0.0478, -0.0184,  0.1069, -0.0597, -0.0248,  0.1247,  0.0171,\n",
      "          0.1184,  0.0820, -0.1119,  0.0596, -0.0968,  0.0754,  0.1022, -0.1512,\n",
      "          0.1764, -0.0095,  0.1121,  0.1453, -0.0691,  0.0002,  0.1156,  0.1589,\n",
      "          0.0977,  0.0212, -0.0535,  0.1600,  0.1244,  0.1086,  0.0724,  0.0808],\n",
      "        [-0.0857,  0.1676, -0.1145, -0.1535, -0.0793, -0.1582, -0.1575,  0.1711,\n",
      "          0.0034, -0.0981, -0.0313, -0.1680,  0.0917,  0.1546,  0.0869,  0.1241,\n",
      "         -0.1561,  0.0898, -0.0232, -0.0467,  0.0577,  0.0561, -0.0423,  0.0785,\n",
      "          0.0231, -0.1377,  0.0881, -0.0745,  0.0029, -0.0556,  0.1610, -0.1676],\n",
      "        [-0.0563, -0.1670, -0.0676,  0.0965, -0.1390,  0.1650,  0.0472,  0.1167,\n",
      "          0.0770,  0.1425,  0.0763, -0.1598,  0.0662, -0.1217,  0.1738,  0.0785,\n",
      "         -0.0562, -0.1719,  0.1456, -0.0778,  0.0429,  0.0763,  0.1731,  0.0787,\n",
      "         -0.0020, -0.0604,  0.0249,  0.0194,  0.0755, -0.0893, -0.0765, -0.0996],\n",
      "        [ 0.0492,  0.0161,  0.0818,  0.0076, -0.0519, -0.0467, -0.0937, -0.1337,\n",
      "         -0.0638, -0.0922, -0.0977,  0.1079, -0.0961,  0.1160, -0.1043, -0.0413,\n",
      "          0.0366, -0.1404, -0.0195, -0.0418,  0.0304, -0.0496,  0.1446,  0.0529,\n",
      "         -0.1487, -0.0245, -0.1745, -0.0841,  0.1472, -0.0207, -0.1719, -0.1728],\n",
      "        [ 0.1235,  0.0634,  0.0167,  0.0592, -0.1500, -0.1207,  0.0609, -0.1121,\n",
      "          0.0463, -0.1361,  0.0041, -0.1181, -0.1172, -0.0343, -0.0675, -0.0889,\n",
      "         -0.1344, -0.1121,  0.0119, -0.0245,  0.0088,  0.0254, -0.0840,  0.1024,\n",
      "         -0.1765,  0.1535,  0.0756,  0.0280, -0.1652,  0.1021,  0.0768,  0.1486],\n",
      "        [ 0.1376,  0.0474, -0.1371,  0.1638, -0.1216,  0.1489, -0.1367,  0.1686,\n",
      "         -0.0271, -0.0746,  0.1142,  0.0867,  0.0372,  0.1157, -0.1109, -0.1668,\n",
      "         -0.0914, -0.1706, -0.1240, -0.1449,  0.1034, -0.0831,  0.1662,  0.0102,\n",
      "          0.0026, -0.0901, -0.0278,  0.1294, -0.1177,  0.0202, -0.1361, -0.0739],\n",
      "        [-0.0080,  0.1030,  0.0684,  0.0109,  0.1071, -0.1206,  0.0927,  0.0755,\n",
      "         -0.0475, -0.1678, -0.0958, -0.0290, -0.1271,  0.0981, -0.0551, -0.1562,\n",
      "          0.0530,  0.0093,  0.0411,  0.0970, -0.0945, -0.0391, -0.0599,  0.0485,\n",
      "         -0.0727, -0.0556,  0.0673,  0.0934,  0.0363,  0.0414, -0.1503,  0.1466],\n",
      "        [-0.0157,  0.0761,  0.0461, -0.0639, -0.0766,  0.0044, -0.0317, -0.0497,\n",
      "         -0.1257,  0.0802,  0.0043,  0.1271, -0.0448,  0.0539,  0.1374,  0.0974,\n",
      "         -0.0135,  0.1286,  0.0546,  0.1647,  0.0232,  0.1692, -0.0106, -0.1473,\n",
      "         -0.1646,  0.0190, -0.1398,  0.0160, -0.1345, -0.0544,  0.1120,  0.0707],\n",
      "        [ 0.1332, -0.1377,  0.1303, -0.0503, -0.1455,  0.1518, -0.0308, -0.1359,\n",
      "         -0.1191, -0.0758, -0.1221, -0.0089, -0.1017,  0.1085, -0.1307, -0.1523,\n",
      "         -0.1262, -0.0938,  0.0984, -0.0992, -0.1013, -0.1125, -0.0750, -0.1265,\n",
      "          0.0982,  0.1212, -0.1261, -0.0631, -0.0005, -0.0708,  0.1563, -0.1286],\n",
      "        [ 0.1604, -0.0912, -0.1137,  0.0673,  0.1668,  0.1141, -0.1150, -0.0128,\n",
      "          0.0800,  0.0738,  0.0286,  0.0978, -0.0325,  0.1343,  0.1035, -0.0116,\n",
      "          0.0211, -0.0837,  0.1275, -0.1543, -0.0849, -0.0329,  0.0042, -0.1172,\n",
      "          0.1308,  0.0308, -0.0362, -0.1408, -0.1482,  0.0204, -0.0864, -0.0442],\n",
      "        [ 0.0994,  0.0841,  0.0273, -0.0715, -0.0255, -0.1679,  0.1251,  0.1342,\n",
      "         -0.1738,  0.0680,  0.0083,  0.0087, -0.0282, -0.1398,  0.1417, -0.0386,\n",
      "          0.0222,  0.1304, -0.1115, -0.0211, -0.1314, -0.1144, -0.0456, -0.1376,\n",
      "         -0.0339, -0.0906,  0.1239,  0.1421, -0.0010,  0.1648, -0.1057,  0.1250],\n",
      "        [ 0.1388, -0.1429, -0.1034, -0.1007, -0.0853, -0.0590,  0.1473, -0.0408,\n",
      "          0.0054,  0.0350,  0.0773, -0.0391,  0.0924,  0.1337, -0.1748,  0.0899,\n",
      "          0.1207, -0.0600, -0.0058,  0.1702, -0.0403, -0.1330, -0.0436,  0.0953,\n",
      "          0.0742, -0.0367, -0.1529, -0.0614, -0.0733, -0.0884, -0.1077, -0.0837],\n",
      "        [-0.1061, -0.1337, -0.0470,  0.1081, -0.0932, -0.1758, -0.1051, -0.1662,\n",
      "         -0.0914,  0.1345,  0.1619,  0.1040, -0.0995,  0.1379,  0.1337, -0.0131,\n",
      "          0.1015, -0.1156, -0.0810, -0.1370,  0.1032, -0.0803,  0.0584,  0.1635,\n",
      "          0.1620, -0.0072, -0.1592, -0.0492,  0.0283,  0.0077, -0.1481, -0.1595]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: fc2.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([ 0.0478, -0.1333,  0.1759,  0.1350, -0.1225,  0.0445, -0.0787, -0.1606,\n",
      "        -0.0109, -0.0144, -0.0403,  0.1743, -0.0099,  0.1211, -0.0942,  0.0218],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: ln.weight\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "Nome: ln.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Nome: qkv_proj.weight\n",
      "Shape: torch.Size([48, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 0.2427, -0.0915,  0.1842, -0.1397,  0.1287, -0.1044, -0.2139,  0.2420,\n",
      "         -0.0106, -0.0258, -0.2136,  0.2033,  0.0562, -0.1888, -0.1166,  0.0991],\n",
      "        [ 0.0013, -0.1807,  0.2352,  0.0744,  0.0663,  0.0517, -0.2144,  0.1869,\n",
      "         -0.0603,  0.2491, -0.1675,  0.2135,  0.0771,  0.0715, -0.2425,  0.0537],\n",
      "        [ 0.2333, -0.1948, -0.0610,  0.1762,  0.0047,  0.1169, -0.1960, -0.0406,\n",
      "          0.1472, -0.1139,  0.0054,  0.0052,  0.2034,  0.2221, -0.0577,  0.1843],\n",
      "        [ 0.0445, -0.0861, -0.1288,  0.0999,  0.1954,  0.1004, -0.0449,  0.0405,\n",
      "         -0.0693,  0.0334,  0.0107, -0.2013, -0.1017, -0.2350, -0.0472, -0.0153],\n",
      "        [ 0.1072,  0.0361, -0.2229, -0.0099, -0.1888, -0.1284,  0.1511,  0.1413,\n",
      "         -0.0729, -0.1766,  0.2027, -0.1237, -0.1497,  0.0336,  0.2196, -0.1302],\n",
      "        [ 0.2159, -0.0382, -0.0405, -0.0079,  0.0433, -0.0652,  0.2235, -0.1393,\n",
      "         -0.1756, -0.0536, -0.0636,  0.1578, -0.1572, -0.2008, -0.0902,  0.1465],\n",
      "        [ 0.0286, -0.1599, -0.2213,  0.2388, -0.0131, -0.0346,  0.0990,  0.0450,\n",
      "         -0.1416,  0.1711,  0.0381,  0.0260,  0.0799,  0.0627,  0.1323, -0.0313],\n",
      "        [ 0.2337, -0.0261, -0.0187,  0.0164, -0.1165, -0.2153, -0.1623, -0.1243,\n",
      "          0.0388, -0.2300,  0.0274, -0.0030,  0.0415,  0.1990,  0.1386,  0.1998],\n",
      "        [ 0.0499,  0.0294,  0.0912,  0.1582,  0.0319, -0.2032,  0.1166, -0.2219,\n",
      "         -0.2140, -0.0031, -0.0432, -0.1912,  0.0940,  0.1454,  0.1891, -0.0483],\n",
      "        [-0.0795,  0.2145,  0.2104,  0.1192, -0.2244,  0.0268,  0.0877,  0.2245,\n",
      "         -0.2256,  0.0249, -0.0678, -0.1105, -0.0677, -0.0025, -0.1999, -0.0821],\n",
      "        [-0.1275, -0.1648, -0.0547,  0.2344, -0.1357,  0.1759,  0.0925, -0.0410,\n",
      "          0.2469, -0.1696,  0.0387, -0.0057,  0.2343, -0.2443, -0.1588,  0.1978],\n",
      "        [ 0.1486, -0.0579,  0.2390, -0.0222, -0.2090,  0.0274, -0.0803,  0.1009,\n",
      "         -0.0277,  0.0461, -0.0902,  0.1933, -0.1683, -0.2140, -0.1320,  0.1386],\n",
      "        [-0.1324,  0.2069, -0.1769,  0.0665, -0.0701,  0.0401, -0.1646, -0.1080,\n",
      "          0.1881, -0.1176,  0.2485,  0.1505,  0.0750, -0.1615,  0.0389, -0.2332],\n",
      "        [ 0.0682, -0.2475, -0.2274, -0.2275,  0.1268,  0.2161, -0.1243,  0.1835,\n",
      "         -0.0233, -0.0619, -0.1744,  0.1301, -0.1249,  0.1589,  0.2422, -0.2201],\n",
      "        [-0.2139,  0.0985,  0.1968,  0.2342, -0.2066, -0.0351,  0.0097,  0.1217,\n",
      "         -0.2419,  0.0773, -0.1763,  0.1698, -0.0033,  0.0206,  0.1436,  0.0614],\n",
      "        [-0.0742,  0.1412,  0.0109,  0.2280, -0.2337,  0.0264, -0.0804,  0.0039,\n",
      "          0.0355, -0.1037,  0.2036, -0.1391,  0.1891,  0.1764,  0.2061, -0.1794],\n",
      "        [ 0.1817, -0.2025, -0.2262,  0.2312,  0.1123, -0.1822,  0.2191, -0.2461,\n",
      "         -0.1251,  0.1067,  0.2337,  0.0655, -0.1595,  0.2188, -0.0799, -0.2447],\n",
      "        [-0.2117,  0.0741, -0.2363, -0.0653, -0.0473,  0.2099, -0.2291, -0.0480,\n",
      "         -0.1557,  0.0055,  0.0527,  0.1605,  0.1222,  0.1039,  0.2467,  0.2185],\n",
      "        [-0.0808,  0.0304,  0.1462,  0.1851,  0.0784, -0.1236, -0.0808,  0.2494,\n",
      "          0.1105, -0.1717,  0.0511, -0.0757, -0.2335, -0.0181,  0.1905, -0.0037],\n",
      "        [-0.1966, -0.1216, -0.1025,  0.1633,  0.1511, -0.0558,  0.0257,  0.1843,\n",
      "         -0.2305,  0.2097, -0.2263,  0.0121,  0.1877, -0.1808, -0.0517, -0.0675],\n",
      "        [ 0.1520, -0.1737,  0.2093,  0.2279, -0.2259,  0.1510, -0.0504, -0.1030,\n",
      "          0.0706, -0.0496, -0.0926, -0.0273, -0.2255, -0.1364, -0.1737,  0.1485],\n",
      "        [ 0.0216, -0.1368,  0.2308,  0.0190, -0.1343, -0.1230,  0.0371, -0.1776,\n",
      "         -0.1021, -0.1339, -0.0112,  0.0758, -0.0660, -0.1013, -0.0583,  0.2328],\n",
      "        [ 0.0038,  0.0037, -0.0599,  0.2358,  0.2021,  0.0932, -0.0769,  0.2448,\n",
      "         -0.0468,  0.1187, -0.1028, -0.0103,  0.0945,  0.1319, -0.0276, -0.0284],\n",
      "        [ 0.1826,  0.1552,  0.1144, -0.1361, -0.0445,  0.1819,  0.1334,  0.1899,\n",
      "         -0.1437, -0.1481,  0.0053,  0.0074,  0.1556, -0.2077,  0.0520,  0.0987],\n",
      "        [-0.2422,  0.1028,  0.2139, -0.1966, -0.1333,  0.2485, -0.2309,  0.2247,\n",
      "         -0.1375, -0.0714,  0.0132, -0.0454, -0.2039, -0.2240, -0.1060,  0.2011],\n",
      "        [ 0.2257,  0.1804, -0.1375,  0.1902,  0.0304, -0.0465, -0.1015, -0.1860,\n",
      "         -0.2499,  0.0098,  0.1197, -0.1489, -0.1797, -0.1409,  0.2463,  0.0565],\n",
      "        [-0.1028,  0.1356, -0.0418, -0.0996,  0.1335,  0.1647, -0.0378, -0.1739,\n",
      "          0.2369,  0.2346,  0.0114,  0.2099,  0.0370, -0.1591, -0.2443,  0.0650],\n",
      "        [-0.1925, -0.1800,  0.2013, -0.1186, -0.0563,  0.0146,  0.0894,  0.0628,\n",
      "          0.0891,  0.2064, -0.2330, -0.2445, -0.0858, -0.1716, -0.1590,  0.0336],\n",
      "        [-0.0374, -0.0520, -0.1597,  0.1981, -0.0485,  0.1653,  0.0957,  0.1526,\n",
      "          0.1445,  0.1508,  0.1888, -0.1828,  0.1897,  0.0587,  0.1975, -0.2344],\n",
      "        [-0.0848,  0.1302, -0.1859,  0.0753, -0.0659,  0.0246, -0.0501,  0.1503,\n",
      "          0.0747, -0.1926,  0.2322, -0.1851,  0.2051,  0.2427, -0.2170,  0.1976],\n",
      "        [ 0.0163,  0.0955,  0.1146,  0.1510, -0.2342, -0.1992, -0.0182,  0.0033,\n",
      "         -0.0168, -0.0275, -0.1997,  0.0756,  0.1934,  0.1395, -0.2406,  0.2004],\n",
      "        [-0.0445, -0.1881, -0.1994, -0.0666,  0.0692, -0.1291, -0.1536,  0.1482,\n",
      "         -0.1252, -0.1108, -0.2179,  0.2435, -0.1345, -0.1798,  0.0809, -0.1399],\n",
      "        [ 0.0100, -0.0654, -0.2271, -0.1273,  0.0949, -0.1476,  0.2185,  0.0177,\n",
      "          0.0553, -0.0399, -0.2134,  0.0400,  0.1020, -0.1052, -0.2098,  0.0101],\n",
      "        [-0.1993, -0.0517,  0.0201,  0.2178,  0.2389, -0.2134, -0.0211, -0.1032,\n",
      "          0.2222, -0.1118,  0.0256, -0.1498, -0.1641,  0.1264, -0.0509,  0.0940],\n",
      "        [-0.1593, -0.1043,  0.1768,  0.1136,  0.1363,  0.1314,  0.1120, -0.0714,\n",
      "          0.1708, -0.1556,  0.0306, -0.1748, -0.2234, -0.1472, -0.0137,  0.0193],\n",
      "        [-0.1806,  0.2473,  0.0710,  0.1134, -0.1074,  0.2091,  0.0326, -0.1659,\n",
      "          0.0750,  0.2061,  0.1305, -0.0600, -0.0279,  0.0090, -0.2105, -0.1317],\n",
      "        [-0.2289,  0.1866, -0.0858,  0.1348, -0.2304, -0.1671,  0.1185,  0.2154,\n",
      "         -0.0987,  0.0931, -0.1267,  0.0177,  0.1902, -0.0918, -0.0968,  0.1231],\n",
      "        [-0.1330, -0.1469, -0.1304, -0.1343,  0.0469,  0.1227,  0.2410, -0.2029,\n",
      "         -0.1168, -0.1628, -0.0985,  0.0054,  0.1715,  0.0207, -0.2057, -0.1706],\n",
      "        [ 0.0929,  0.2458,  0.0274, -0.1710,  0.0365,  0.0917, -0.0632, -0.2466,\n",
      "          0.1208, -0.0144,  0.0168,  0.1872, -0.1766, -0.0055,  0.0706, -0.2117],\n",
      "        [-0.1345,  0.0331,  0.2011,  0.2310,  0.1409, -0.0429, -0.0923, -0.1847,\n",
      "         -0.0979,  0.0359, -0.2360,  0.1605,  0.0619, -0.2268,  0.2299,  0.1982],\n",
      "        [ 0.2184,  0.1045,  0.1677,  0.0381,  0.1499, -0.1499, -0.0558,  0.2244,\n",
      "          0.0384,  0.0221, -0.1930, -0.0357, -0.0584,  0.1164,  0.2139, -0.0616],\n",
      "        [-0.1531,  0.0607,  0.1910,  0.1255, -0.1683, -0.1017,  0.2461, -0.0993,\n",
      "         -0.1988,  0.1918, -0.0548,  0.0827,  0.2087,  0.0345, -0.1493,  0.1045],\n",
      "        [ 0.0275, -0.0392, -0.0708,  0.0651, -0.0076,  0.1828,  0.1298,  0.2396,\n",
      "          0.1074, -0.1793,  0.2089,  0.1480, -0.0642, -0.1436,  0.0018, -0.2204],\n",
      "        [ 0.0370, -0.1220, -0.1333, -0.2461,  0.2336, -0.1582, -0.0419, -0.2034,\n",
      "         -0.1174,  0.0607,  0.1493,  0.1412, -0.1649, -0.1290, -0.1150,  0.0502],\n",
      "        [ 0.0799, -0.2493, -0.0470, -0.2181, -0.1222,  0.0487, -0.2360,  0.2160,\n",
      "         -0.0530,  0.1794, -0.2023,  0.2238, -0.1382, -0.0910,  0.1124, -0.1527],\n",
      "        [-0.2373, -0.0314, -0.2022, -0.1025, -0.1831,  0.0119, -0.0252, -0.0931,\n",
      "         -0.2167, -0.1441,  0.1632, -0.2201,  0.0456, -0.0378, -0.0885,  0.0100],\n",
      "        [ 0.0680,  0.1506,  0.1214,  0.0109,  0.0707,  0.0094,  0.2316, -0.2290,\n",
      "          0.0389, -0.2133,  0.0860,  0.1043,  0.0471,  0.0708, -0.2035, -0.0299],\n",
      "        [-0.2102, -0.1518,  0.1542,  0.1361, -0.0686, -0.1129,  0.0351,  0.2238,\n",
      "         -0.0214, -0.0511, -0.0116,  0.2156, -0.1036,  0.0490,  0.0892, -0.0489]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: qkv_proj.bias\n",
      "Shape: torch.Size([48])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores: Parameter containing:\n",
      "tensor([-0.1917,  0.1750,  0.1953, -0.0810,  0.1231,  0.0821,  0.1502, -0.1653,\n",
      "         0.1831, -0.0937, -0.2274, -0.1788,  0.0564,  0.1008, -0.0743, -0.0977,\n",
      "         0.1816, -0.2155,  0.1717, -0.0980, -0.1776, -0.1639, -0.2377,  0.0952,\n",
      "         0.0115,  0.1213, -0.0215,  0.0009,  0.1063, -0.1983,  0.0089, -0.1568,\n",
      "        -0.1502,  0.0312, -0.0082, -0.2336, -0.1698,  0.0581,  0.2342, -0.2106,\n",
      "         0.2353,  0.1917,  0.2035, -0.0433,  0.1964, -0.2007, -0.1430,  0.0583],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: out_proj.weight\n",
      "Shape: torch.Size([16, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[ 0.2374,  0.2366,  0.2150,  0.1221,  0.2267, -0.1625,  0.2420,  0.0584,\n",
      "         -0.2139, -0.0107,  0.0479,  0.0275, -0.2230, -0.1187,  0.0731, -0.1701],\n",
      "        [ 0.1832,  0.0262,  0.0428, -0.1969, -0.1900,  0.2169,  0.1331,  0.2452,\n",
      "          0.2212,  0.2092,  0.1304, -0.1415, -0.0077, -0.1871,  0.2109,  0.0123],\n",
      "        [-0.1489, -0.1997,  0.2301,  0.0735,  0.1892,  0.1630,  0.0366, -0.1660,\n",
      "         -0.1396,  0.1309,  0.0201,  0.0393,  0.1706, -0.1206,  0.1996,  0.1155],\n",
      "        [-0.0664, -0.1619, -0.0999,  0.1989,  0.0715, -0.1816,  0.1337, -0.0159,\n",
      "         -0.1792,  0.0218, -0.2006, -0.1263, -0.1705,  0.0709, -0.0585,  0.1686],\n",
      "        [-0.1530,  0.1190,  0.2389,  0.1845, -0.0375,  0.0036, -0.1247,  0.1906,\n",
      "          0.2223,  0.1430, -0.1772,  0.1098,  0.0414,  0.2177,  0.0059, -0.1384],\n",
      "        [-0.2317,  0.0721, -0.1383, -0.1643, -0.0991,  0.1376,  0.1122, -0.0305,\n",
      "         -0.1677, -0.1394, -0.1666,  0.1937, -0.1484,  0.1597, -0.1731, -0.1796],\n",
      "        [ 0.0350, -0.0678, -0.1445,  0.2441,  0.0209,  0.0121, -0.1336, -0.1968,\n",
      "          0.2187, -0.0416,  0.0970,  0.0064, -0.1002,  0.0088, -0.2439, -0.1568],\n",
      "        [-0.0636,  0.0261,  0.1766, -0.0135, -0.2189, -0.1586,  0.0949, -0.2189,\n",
      "          0.0790, -0.1495, -0.2051,  0.1909,  0.2199, -0.0971,  0.1711,  0.1103],\n",
      "        [ 0.1560,  0.2274, -0.0913,  0.1294,  0.1414, -0.0223, -0.2036,  0.0040,\n",
      "          0.0077,  0.1281,  0.2211,  0.0512, -0.1057, -0.1801, -0.1435, -0.0508],\n",
      "        [-0.0491, -0.1179, -0.0885, -0.1223, -0.0521,  0.0414, -0.0369,  0.1679,\n",
      "         -0.1038, -0.2144, -0.0255, -0.0886, -0.2217, -0.1910,  0.0797, -0.1879],\n",
      "        [-0.0600, -0.0986, -0.0068, -0.0692,  0.2479, -0.1980,  0.1595,  0.0098,\n",
      "          0.1107, -0.0594, -0.0780,  0.2123,  0.0398,  0.1278, -0.1959,  0.0499],\n",
      "        [-0.1126,  0.2135, -0.1695,  0.2337, -0.1981, -0.0762,  0.0006,  0.1550,\n",
      "         -0.0328, -0.0353,  0.2273, -0.0732, -0.1320,  0.2051,  0.2294, -0.1603],\n",
      "        [ 0.2092,  0.2104,  0.1714, -0.1604, -0.0994, -0.1750,  0.1160, -0.0122,\n",
      "          0.0140, -0.1755, -0.0334,  0.1114,  0.0140, -0.1771, -0.2423, -0.1978],\n",
      "        [ 0.0339,  0.0768, -0.2043, -0.0301,  0.1654,  0.1965,  0.0888,  0.0133,\n",
      "          0.1062, -0.1962, -0.0952, -0.1711, -0.2161,  0.2093, -0.0147,  0.1878],\n",
      "        [-0.0152, -0.1429, -0.2309, -0.1204,  0.0976,  0.0772, -0.0369, -0.0565,\n",
      "          0.1918, -0.1369,  0.1477, -0.1606,  0.0602, -0.1606, -0.1181, -0.0061],\n",
      "        [ 0.0913, -0.1117,  0.0371, -0.1051,  0.1247,  0.0632, -0.0291, -0.0899,\n",
      "         -0.2144,  0.0652,  0.0302, -0.1478, -0.1089,  0.1427,  0.0311, -0.1349]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: out_proj.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([ 0.1693,  0.1239,  0.0883, -0.1379, -0.0829, -0.1564, -0.1493, -0.1665,\n",
      "        -0.0836, -0.1052,  0.0946, -0.0076, -0.0601, -0.0850, -0.0601,  0.1400],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: final_ln.weight\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: final_ln.bias\n",
      "Shape: torch.Size([16])\n",
      "Valores: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: lm_head.weight\n",
      "Shape: torch.Size([209, 16])\n",
      "Valores: Parameter containing:\n",
      "tensor([[-0.1198, -0.2181,  0.2135,  ...,  0.0408,  0.0308, -0.0640],\n",
      "        [ 0.1135,  0.2380,  0.1625,  ..., -0.0415,  0.0683,  0.1630],\n",
      "        [ 0.0474,  0.1264,  0.1168,  ..., -0.2221,  0.1041, -0.1659],\n",
      "        ...,\n",
      "        [ 0.2440, -0.1776, -0.1266,  ...,  0.0784, -0.1439,  0.0849],\n",
      "        [-0.1129,  0.0621,  0.2371,  ..., -0.0281,  0.0746, -0.0022],\n",
      "        [ 0.0230,  0.0944, -0.1983,  ..., -0.0160,  0.0411, -0.2359]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Nome: lm_head.bias\n",
      "Shape: torch.Size([209])\n",
      "Valores: Parameter containing:\n",
      "tensor([ 1.0286e-01, -8.8576e-02,  1.1573e-02, -7.1857e-02, -2.4243e-01,\n",
      "        -1.3767e-01,  1.7435e-01, -9.0039e-02,  1.0233e-01, -2.1383e-01,\n",
      "         2.4000e-01,  2.1776e-01, -2.4220e-01, -1.2043e-01,  2.1706e-01,\n",
      "         2.0515e-02,  1.1963e-01,  2.4385e-01,  6.0864e-02,  2.4672e-02,\n",
      "        -2.1644e-01, -1.4930e-01,  2.4026e-01, -1.3782e-01,  1.8241e-01,\n",
      "        -6.1912e-02, -2.0008e-01,  6.0496e-02,  1.2042e-01, -8.3077e-03,\n",
      "        -8.4037e-02, -1.0281e-01,  6.7417e-02,  6.2341e-02,  1.0475e-01,\n",
      "        -7.8908e-02,  1.8716e-01, -2.4482e-02,  1.5972e-01, -1.9296e-02,\n",
      "        -1.1770e-01,  1.3236e-01, -1.3188e-01, -1.8626e-01, -1.6429e-01,\n",
      "        -2.1044e-01, -1.3200e-01,  1.8569e-01,  2.0082e-01, -2.3444e-01,\n",
      "        -2.2800e-01,  2.0709e-01, -1.9102e-01,  1.2679e-01, -2.4365e-01,\n",
      "         3.9494e-04, -5.6048e-02, -7.2636e-02,  2.2160e-01, -2.4487e-01,\n",
      "         1.6776e-01, -8.8333e-03,  3.3799e-02, -1.1481e-02, -2.1257e-01,\n",
      "        -2.3618e-01, -7.0930e-05, -9.3363e-02,  1.3756e-01, -1.0433e-01,\n",
      "         2.0991e-01, -1.1064e-01, -1.6573e-01,  1.7836e-01,  1.9090e-01,\n",
      "        -2.3609e-01,  2.4013e-01, -1.1206e-01, -6.3079e-02, -8.2244e-02,\n",
      "         1.8232e-01, -3.1377e-02,  6.5935e-02,  1.0764e-01,  1.4398e-01,\n",
      "         7.2346e-02, -7.5157e-02, -1.0892e-01, -8.7915e-02,  7.0006e-02,\n",
      "        -2.3529e-01,  1.3775e-01, -1.1407e-02, -1.7080e-01,  1.8457e-01,\n",
      "         2.5943e-02,  2.1957e-01,  1.8363e-01, -1.3539e-02,  3.3722e-02,\n",
      "        -7.7184e-02, -7.6018e-02, -1.7863e-01, -3.1909e-02,  2.1278e-01,\n",
      "        -1.9003e-01, -6.9843e-02, -9.8172e-02,  1.7913e-01, -9.9127e-02,\n",
      "        -8.1935e-02, -2.2184e-01,  1.6720e-01, -7.3832e-02,  1.1422e-01,\n",
      "        -4.8804e-02, -4.2526e-02, -3.0639e-02, -2.1929e-01,  1.2476e-01,\n",
      "        -1.1630e-01,  1.9005e-01,  8.2167e-02, -1.6887e-01,  6.3935e-02,\n",
      "         3.2365e-02, -1.1103e-02,  2.4907e-01, -2.2242e-02,  3.9799e-02,\n",
      "        -3.8708e-02,  2.2924e-01,  2.1251e-01, -2.3672e-01,  1.2534e-01,\n",
      "        -1.3537e-01,  1.4204e-01, -1.4335e-01, -7.7179e-02,  8.5896e-02,\n",
      "        -1.8058e-01, -9.2898e-02,  8.2421e-02,  1.8426e-01, -1.2422e-01,\n",
      "        -1.0403e-01, -2.7924e-02, -4.8416e-02, -2.4899e-01, -9.6740e-02,\n",
      "        -5.2844e-02, -7.9521e-02,  1.8662e-01,  2.1049e-01, -1.1020e-01,\n",
      "        -8.0213e-02, -1.3632e-02, -3.2776e-02,  7.3857e-02, -2.3958e-01,\n",
      "        -1.8019e-01, -1.7725e-01, -2.3445e-02, -2.3280e-01,  2.3308e-01,\n",
      "         1.2070e-01, -7.7756e-02,  1.0581e-01,  5.4879e-02,  1.2515e-02,\n",
      "         1.2685e-01,  1.9363e-01,  1.3157e-01,  1.4483e-01,  1.0557e-01,\n",
      "         1.2256e-01,  1.3572e-01,  1.5872e-01,  1.2006e-02,  1.9575e-01,\n",
      "        -1.0654e-01,  9.1535e-02,  1.7858e-01,  7.7188e-02, -1.5102e-01,\n",
      "         3.8058e-02, -1.1648e-01, -6.1748e-02, -1.3339e-01, -2.2470e-01,\n",
      "         7.9713e-02,  4.5229e-02, -1.3470e-01, -1.6737e-01, -2.2333e-01,\n",
      "        -1.2344e-03, -1.2200e-01, -2.9599e-02, -5.6792e-02, -2.6680e-02,\n",
      "        -1.6491e-01,  4.1120e-02,  4.1797e-02,  1.2337e-01, -5.7936e-02,\n",
      "        -1.9531e-01,  1.0471e-03,  3.4162e-02,  1.4366e-01],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Nome: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Valores: {param}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539ba1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (wte): Embedding(209, 16)\n",
      "  (wpe): Embedding(16, 16)\n",
      "  (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (gelu): GELU(approximate='tanh')\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (qkv_proj): Linear(in_features=16, out_features=48, bias=True)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (final_ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=16, out_features=209, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45d1f33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abrir'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "953f9bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'eu',\n",
       " 1: 'você',\n",
       " 2: 'ele',\n",
       " 3: 'ela',\n",
       " 4: 'nós',\n",
       " 5: 'vocês',\n",
       " 6: 'eles',\n",
       " 7: 'elas',\n",
       " 8: 'me',\n",
       " 9: 'te',\n",
       " 10: 'se',\n",
       " 11: 'nos',\n",
       " 12: 'vos',\n",
       " 13: 'lhe',\n",
       " 14: 'lhes',\n",
       " 15: 'mim',\n",
       " 16: 'ti',\n",
       " 17: 'si',\n",
       " 18: 'ser',\n",
       " 19: 'estar',\n",
       " 20: 'ter',\n",
       " 21: 'haver',\n",
       " 22: 'fazer',\n",
       " 23: 'poder',\n",
       " 24: 'dizer',\n",
       " 25: 'ver',\n",
       " 26: 'dar',\n",
       " 27: 'saber',\n",
       " 28: 'querer',\n",
       " 29: 'chegar',\n",
       " 30: 'passar',\n",
       " 31: 'dever',\n",
       " 32: 'ficar',\n",
       " 33: 'deixar',\n",
       " 34: 'pensar',\n",
       " 35: 'vir',\n",
       " 36: 'conhecer',\n",
       " 37: 'casa',\n",
       " 38: 'tempo',\n",
       " 39: 'dia',\n",
       " 40: 'mundo',\n",
       " 41: 'homem',\n",
       " 42: 'mulher',\n",
       " 43: 'vida',\n",
       " 44: 'mão',\n",
       " 45: 'olho',\n",
       " 46: 'palavra',\n",
       " 47: 'caminho',\n",
       " 48: 'gato',\n",
       " 49: 'cachorro',\n",
       " 50: 'carro',\n",
       " 51: 'livro',\n",
       " 52: 'porta',\n",
       " 53: 'rua',\n",
       " 154: 'trabalho',\n",
       " 55: 'dinheiro',\n",
       " 56: 'noite',\n",
       " 57: 'bom',\n",
       " 58: 'mau',\n",
       " 59: 'feliz',\n",
       " 60: 'triste',\n",
       " 61: 'grande',\n",
       " 62: 'pequeno',\n",
       " 63: 'novo',\n",
       " 64: 'velho',\n",
       " 65: 'forte',\n",
       " 66: 'fraco',\n",
       " 67: 'belo',\n",
       " 68: 'feio',\n",
       " 69: 'inteligente',\n",
       " 70: 'rápido',\n",
       " 71: 'lento',\n",
       " 72: 'em',\n",
       " 73: 'de',\n",
       " 74: 'com',\n",
       " 75: 'por',\n",
       " 76: 'para',\n",
       " 77: 'sobre',\n",
       " 78: 'entre',\n",
       " 79: 'até',\n",
       " 80: 'após',\n",
       " 131: 'antes',\n",
       " 82: 'sem',\n",
       " 83: 'sob',\n",
       " 84: 'contra',\n",
       " 85: 'durante',\n",
       " 86: 'perante',\n",
       " 87: 'trás',\n",
       " 88: 'o',\n",
       " 89: 'a',\n",
       " 90: 'os',\n",
       " 91: 'as',\n",
       " 104: 'um',\n",
       " 93: 'uma',\n",
       " 94: 'uns',\n",
       " 95: 'umas',\n",
       " 96: 'do',\n",
       " 97: 'da',\n",
       " 98: 'dos',\n",
       " 99: 'das',\n",
       " 100: 'este',\n",
       " 101: 'esta',\n",
       " 102: 'estes',\n",
       " 103: 'estas',\n",
       " 105: 'dois',\n",
       " 106: 'três',\n",
       " 107: 'quatro',\n",
       " 108: 'cinco',\n",
       " 109: 'seis',\n",
       " 110: 'sete',\n",
       " 111: 'oito',\n",
       " 112: 'nove',\n",
       " 113: 'dez',\n",
       " 114: 'vinte',\n",
       " 115: 'trinta',\n",
       " 116: 'cem',\n",
       " 117: 'mil',\n",
       " 118: 'milhão',\n",
       " 119: 'sim',\n",
       " 120: 'não',\n",
       " 121: 'talvez',\n",
       " 122: 'hoje',\n",
       " 123: 'amanhã',\n",
       " 124: 'ontem',\n",
       " 125: 'agora',\n",
       " 126: 'sempre',\n",
       " 127: 'nunca',\n",
       " 128: 'já',\n",
       " 129: 'ainda',\n",
       " 130: 'depois',\n",
       " 132: 'aqui',\n",
       " 133: 'ali',\n",
       " 134: 'lá',\n",
       " 135: 'longe',\n",
       " 136: 'perto',\n",
       " 137: 'escola',\n",
       " 138: 'cidade',\n",
       " 139: 'país',\n",
       " 140: 'terra',\n",
       " 141: 'céu',\n",
       " 142: 'mar',\n",
       " 143: 'praia',\n",
       " 144: 'montanha',\n",
       " 145: 'rio',\n",
       " 146: 'floresta',\n",
       " 147: 'livraria',\n",
       " 148: 'mercado',\n",
       " 149: 'amigo',\n",
       " 150: 'amiga',\n",
       " 151: 'criança',\n",
       " 152: 'professor',\n",
       " 153: 'aluno',\n",
       " 155: 'pessoa',\n",
       " 156: 'família',\n",
       " 157: 'história',\n",
       " 158: 'amor',\n",
       " 159: 'vou',\n",
       " 160: 'vai',\n",
       " 161: 'vamos',\n",
       " 162: 'vão',\n",
       " 163: 'fui',\n",
       " 164: 'foi',\n",
       " 165: 'foram',\n",
       " 166: 'estou',\n",
       " 167: 'está',\n",
       " 168: 'estamos',\n",
       " 169: 'estão',\n",
       " 170: 'gostar',\n",
       " 171: 'amar',\n",
       " 172: 'odiar',\n",
       " 173: 'correr',\n",
       " 174: 'andar',\n",
       " 175: 'viajar',\n",
       " 176: 'sorrir',\n",
       " 177: 'chorar',\n",
       " 178: 'brincar',\n",
       " 179: 'estudar',\n",
       " 180: 'beber',\n",
       " 181: 'comer',\n",
       " 182: 'dormir',\n",
       " 183: 'acordar',\n",
       " 184: 'abrir',\n",
       " 185: 'fechar',\n",
       " 186: 'comprar',\n",
       " 187: 'vender',\n",
       " 188: 'ligar',\n",
       " 189: 'desligar',\n",
       " 190: 'e',\n",
       " 191: 'mas',\n",
       " 192: 'ou',\n",
       " 193: 'porque',\n",
       " 194: 'que',\n",
       " 195: 'como',\n",
       " 196: 'quando',\n",
       " 197: 'onde',\n",
       " 198: 'quem',\n",
       " 199: 'qual',\n",
       " 200: 'podem',\n",
       " 201: 'explorar',\n",
       " 202: 'universidade',\n",
       " 203: 'experiência',\n",
       " 204: 'interessante',\n",
       " 205: 'no',\n",
       " 206: 'na',\n",
       " 207: 'felizes',\n",
       " 208: 'tivemos'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5d72df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: o gato pequeno vamos\n",
      "Passo 2: o gato pequeno vamos ali\n",
      "Passo 3: o gato pequeno vamos ali um\n",
      "Passo 4: o gato pequeno vamos ali um após\n",
      "Passo 5: o gato pequeno vamos ali um após escola\n",
      "Passo 6: o gato pequeno vamos ali um após escola experiência\n",
      "Passo 7: o gato pequeno vamos ali um após escola experiência conhecer\n",
      "Passo 8: o gato pequeno vamos ali um após escola experiência conhecer escola\n",
      "Passo 9: o gato pequeno vamos ali um após escola experiência conhecer escola amar\n",
      "Passo 10: o gato pequeno vamos ali um após escola experiência conhecer escola amar a\n",
      "Passo 11: o gato pequeno vamos ali um após escola experiência conhecer escola amar a felizes\n",
      "Passo 12: o gato pequeno vamos ali um após escola experiência conhecer escola amar a felizes os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['o', 'gato', 'pequeno']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50974d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
