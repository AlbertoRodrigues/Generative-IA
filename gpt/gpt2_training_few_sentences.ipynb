{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708f0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from gpt_model import GPT\n",
    "from vocab import tokens\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f3ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocês']\n",
      "podem\n",
      "['vocês', 'podem']\n",
      "explorar\n",
      "['vocês', 'podem', 'explorar']\n",
      "a\n",
      "['vocês', 'podem', 'explorar', 'a']\n",
      "floresta\n",
      "['vocês', 'podem', 'explorar', 'a', 'floresta']\n",
      "durante\n",
      "['vocês', 'podem', 'explorar', 'a', 'floresta', 'durante']\n",
      "o\n",
      "['vocês', 'podem', 'explorar', 'a', 'floresta', 'durante', 'o']\n",
      "dia\n",
      "['ele']\n",
      "vai\n",
      "['ele', 'vai']\n",
      "vender\n",
      "['ele', 'vai', 'vender']\n",
      "um\n",
      "['ele', 'vai', 'vender', 'um']\n",
      "livro\n",
      "['ele', 'vai', 'vender', 'um', 'livro']\n",
      "no\n",
      "['ele', 'vai', 'vender', 'um', 'livro', 'no']\n",
      "mercado\n",
      "['ele', 'vai', 'vender', 'um', 'livro', 'no', 'mercado']\n",
      "amanhã\n",
      "['nós']\n",
      "estamos\n",
      "['nós', 'estamos']\n",
      "felizes\n",
      "['nós', 'estamos', 'felizes']\n",
      "porque\n",
      "['nós', 'estamos', 'felizes', 'porque']\n",
      "tivemos\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos']\n",
      "uma\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma']\n",
      "experiência\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência']\n",
      "interessante\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência', 'interessante']\n",
      "na\n",
      "['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência', 'interessante', 'na']\n",
      "universidade\n"
     ]
    }
   ],
   "source": [
    "# Lista de frases como listas de tokens de treinamento\n",
    "all_tokens = [\n",
    "    [\"vocês\", \"podem\",   \"explorar\", \"a\", \"floresta\", \"durante\", \"o\", \"dia\"],\n",
    "    [\"ele\",   \"vai\",     \"vender\",   \"um\",\"livro\",    \"no\",      \"mercado\",\"amanhã\"],\n",
    "    [\"nós\",   \"estamos\", \"felizes\",  \"porque\",\"tivemos\",\"uma\",\"experiência\",\"interessante\",\"na\",\"universidade\"]\n",
    "]\n",
    "\n",
    "\n",
    "# Inicializa as listas de inputs e targets\n",
    "inputs, targets = [], []\n",
    "\n",
    "# Para cada frase, acumula pares (contexto → próximo token)\n",
    "for tokens in all_tokens:\n",
    "    for i in range(1, len(tokens)):\n",
    "        # contexto: tokens até a posição i (excluindo i)\n",
    "        inputs.append(tokens[:i])\n",
    "        print(tokens[:i])\n",
    "        # target: token na posição i\n",
    "        targets.append(tokens[i])\n",
    "        print(tokens[i])\n",
    "\n",
    "# Exemplo de saída:\n",
    "# inputs  = [[\"o\"], [\"o\",\"gato\"], [\"o\",\"gato\",\"dorme\"], ..., [\"nós\",\"vai\",\"para\",\"escola\"]]\n",
    "# targets = [\"gato\",\"dorme\",\"em\", ..., \"amanhã\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a984c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(inputs, targets))\n",
    "random.shuffle(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2470c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['vocês', 'podem'], 'explorar'),\n",
       " (['ele'], 'vai'),\n",
       " (['nós', 'estamos', 'felizes'], 'porque'),\n",
       " (['vocês'], 'podem'),\n",
       " (['nós', 'estamos', 'felizes', 'porque', 'tivemos'], 'uma'),\n",
       " (['ele', 'vai', 'vender', 'um', 'livro', 'no', 'mercado'], 'amanhã'),\n",
       " (['vocês', 'podem', 'explorar', 'a', 'floresta', 'durante', 'o'], 'dia'),\n",
       " (['nós',\n",
       "   'estamos',\n",
       "   'felizes',\n",
       "   'porque',\n",
       "   'tivemos',\n",
       "   'uma',\n",
       "   'experiência',\n",
       "   'interessante',\n",
       "   'na'],\n",
       "  'universidade'),\n",
       " (['nós',\n",
       "   'estamos',\n",
       "   'felizes',\n",
       "   'porque',\n",
       "   'tivemos',\n",
       "   'uma',\n",
       "   'experiência',\n",
       "   'interessante'],\n",
       "  'na'),\n",
       " (['ele', 'vai', 'vender', 'um'], 'livro'),\n",
       " (['ele', 'vai', 'vender', 'um', 'livro'], 'no'),\n",
       " (['ele', 'vai', 'vender'], 'um'),\n",
       " (['vocês', 'podem', 'explorar', 'a', 'floresta', 'durante'], 'o'),\n",
       " (['vocês', 'podem', 'explorar', 'a', 'floresta'], 'durante'),\n",
       " (['ele', 'vai', 'vender', 'um', 'livro', 'no'], 'mercado'),\n",
       " (['vocês', 'podem', 'explorar', 'a'], 'floresta'),\n",
       " (['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma', 'experiência'],\n",
       "  'interessante'),\n",
       " (['nós'], 'estamos'),\n",
       " (['nós', 'estamos', 'felizes', 'porque'], 'tivemos'),\n",
       " (['vocês', 'podem', 'explorar'], 'a'),\n",
       " (['ele', 'vai'], 'vender'),\n",
       " (['nós', 'estamos', 'felizes', 'porque', 'tivemos', 'uma'], 'experiência'),\n",
       " (['nós', 'estamos'], 'felizes')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66708130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. modelo + otimizador ───────────────────────────────────────────────\n",
    "#device      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model       = GPT()\n",
    "optimizer   = AdamW(model.parameters(), lr=1e-3)\n",
    "criterion   = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb4c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/100 — Loss: 4.4259\n",
      "Época 2/100 — Loss: 4.1897\n",
      "Época 3/100 — Loss: 3.9508\n",
      "Época 4/100 — Loss: 3.7095\n",
      "Época 5/100 — Loss: 3.4680\n",
      "Época 6/100 — Loss: 3.2295\n",
      "Época 7/100 — Loss: 2.9980\n",
      "Época 8/100 — Loss: 2.7767\n",
      "Época 9/100 — Loss: 2.5683\n",
      "Época 10/100 — Loss: 2.3740\n",
      "Época 11/100 — Loss: 2.1939\n",
      "Época 12/100 — Loss: 2.0273\n",
      "Época 13/100 — Loss: 1.8730\n",
      "Época 14/100 — Loss: 1.7300\n",
      "Época 15/100 — Loss: 1.5972\n",
      "Época 16/100 — Loss: 1.4739\n",
      "Época 17/100 — Loss: 1.3594\n",
      "Época 18/100 — Loss: 1.2531\n",
      "Época 19/100 — Loss: 1.1545\n",
      "Época 20/100 — Loss: 1.0633\n",
      "Época 21/100 — Loss: 0.9790\n",
      "Época 22/100 — Loss: 0.9012\n",
      "Época 23/100 — Loss: 0.8296\n",
      "Época 24/100 — Loss: 0.7637\n",
      "Época 25/100 — Loss: 0.7032\n",
      "Época 26/100 — Loss: 0.6478\n",
      "Época 27/100 — Loss: 0.5970\n",
      "Época 28/100 — Loss: 0.5507\n",
      "Época 29/100 — Loss: 0.5084\n",
      "Época 30/100 — Loss: 0.4699\n",
      "Época 31/100 — Loss: 0.4348\n",
      "Época 32/100 — Loss: 0.4028\n",
      "Época 33/100 — Loss: 0.3738\n",
      "Época 34/100 — Loss: 0.3473\n",
      "Época 35/100 — Loss: 0.3232\n",
      "Época 36/100 — Loss: 0.3013\n",
      "Época 37/100 — Loss: 0.2813\n",
      "Época 38/100 — Loss: 0.2630\n",
      "Época 39/100 — Loss: 0.2462\n",
      "Época 40/100 — Loss: 0.2309\n",
      "Época 41/100 — Loss: 0.2168\n",
      "Época 42/100 — Loss: 0.2039\n",
      "Época 43/100 — Loss: 0.1921\n",
      "Época 44/100 — Loss: 0.1812\n",
      "Época 45/100 — Loss: 0.1711\n",
      "Época 46/100 — Loss: 0.1618\n",
      "Época 47/100 — Loss: 0.1532\n",
      "Época 48/100 — Loss: 0.1452\n",
      "Época 49/100 — Loss: 0.1379\n",
      "Época 50/100 — Loss: 0.1310\n",
      "Época 51/100 — Loss: 0.1246\n",
      "Época 52/100 — Loss: 0.1187\n",
      "Época 53/100 — Loss: 0.1132\n",
      "Época 54/100 — Loss: 0.1080\n",
      "Época 55/100 — Loss: 0.1031\n",
      "Época 56/100 — Loss: 0.0986\n",
      "Época 57/100 — Loss: 0.0944\n",
      "Época 58/100 — Loss: 0.0904\n",
      "Época 59/100 — Loss: 0.0867\n",
      "Época 60/100 — Loss: 0.0831\n",
      "Época 61/100 — Loss: 0.0798\n",
      "Época 62/100 — Loss: 0.0767\n",
      "Época 63/100 — Loss: 0.0737\n",
      "Época 64/100 — Loss: 0.0710\n",
      "Época 65/100 — Loss: 0.0683\n",
      "Época 66/100 — Loss: 0.0658\n",
      "Época 67/100 — Loss: 0.0635\n",
      "Época 68/100 — Loss: 0.0612\n",
      "Época 69/100 — Loss: 0.0591\n",
      "Época 70/100 — Loss: 0.0571\n",
      "Época 71/100 — Loss: 0.0552\n",
      "Época 72/100 — Loss: 0.0533\n",
      "Época 73/100 — Loss: 0.0516\n",
      "Época 74/100 — Loss: 0.0499\n",
      "Época 75/100 — Loss: 0.0483\n",
      "Época 76/100 — Loss: 0.0468\n",
      "Época 77/100 — Loss: 0.0454\n",
      "Época 78/100 — Loss: 0.0440\n",
      "Época 79/100 — Loss: 0.0427\n",
      "Época 80/100 — Loss: 0.0414\n",
      "Época 81/100 — Loss: 0.0402\n",
      "Época 82/100 — Loss: 0.0391\n",
      "Época 83/100 — Loss: 0.0379\n",
      "Época 84/100 — Loss: 0.0369\n",
      "Época 85/100 — Loss: 0.0358\n",
      "Época 86/100 — Loss: 0.0349\n",
      "Época 87/100 — Loss: 0.0339\n",
      "Época 88/100 — Loss: 0.0330\n",
      "Época 89/100 — Loss: 0.0321\n",
      "Época 90/100 — Loss: 0.0313\n",
      "Época 91/100 — Loss: 0.0305\n",
      "Época 92/100 — Loss: 0.0297\n",
      "Época 93/100 — Loss: 0.0289\n",
      "Época 94/100 — Loss: 0.0282\n",
      "Época 95/100 — Loss: 0.0275\n",
      "Época 96/100 — Loss: 0.0268\n",
      "Época 97/100 — Loss: 0.0262\n",
      "Época 98/100 — Loss: 0.0255\n",
      "Época 99/100 — Loss: 0.0249\n",
      "Época 100/100 — Loss: 0.0243\n"
     ]
    }
   ],
   "source": [
    "# ── 2. loop de treino ────────────────────────────────────────────────────\n",
    "num_epochs   = 100\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for input_tokens, target_token in train_data:          # ↳ seu dataloader/iterável\n",
    "        # (opcional) envie tudo para GPU, se estiver usando\n",
    "        input_tokens = list(input_tokens)                  # garante lista mutável\n",
    "        model.init_tokens(input_tokens)                    # define contexto atual\n",
    "\n",
    "        # transforma tokens em índices para o forward\n",
    "        model.tokens_idx(input_tokens)                     # cria self.indices_tensor\n",
    "\n",
    "        # ── forward ──────────────────────────────────────\n",
    "        last_hidden = model.forward()                      # (n_embd,)\n",
    "        logits      = model.predict_logits(last_hidden)    # (vocab_size,)\n",
    "\n",
    "        # ── loss ────────────────────────────────────────\n",
    "        target_idx  = torch.tensor([model.tokens_vocab[target_token]],\n",
    "                                   dtype=torch.long,\n",
    "                                   device=logits.device)   # shape (1,)\n",
    "        loss        = criterion(logits.unsqueeze(0), target_idx)\n",
    "\n",
    "        # ── backward + update ───────────────────────────\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # média da loss na época\n",
    "    avg_loss = epoch_loss / len(train_data)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Época {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d0aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_tokens([\"vocês\", \"podem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888f1721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explorar'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79cf061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: vocês podem explorar\n",
      "Passo 2: vocês podem explorar a\n",
      "Passo 3: vocês podem explorar a floresta\n",
      "Passo 4: vocês podem explorar a floresta durante\n",
      "Passo 5: vocês podem explorar a floresta durante o\n",
      "Passo 6: vocês podem explorar a floresta durante o dia\n",
      "Passo 7: vocês podem explorar a floresta durante o dia estamos\n",
      "Passo 8: vocês podem explorar a floresta durante o dia estamos felizes\n",
      "Passo 9: vocês podem explorar a floresta durante o dia estamos felizes um\n",
      "Passo 10: vocês podem explorar a floresta durante o dia estamos felizes um livro\n",
      "Passo 11: vocês podem explorar a floresta durante o dia estamos felizes um livro estamos\n",
      "Passo 12: vocês podem explorar a floresta durante o dia estamos felizes um livro estamos felizes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vocês', 'podem']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb7020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss(torch.tensor([[0.1, 2.9]]), torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8371c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Converte tokens → índices\n",
    "inputs_idx  = [[vocab[t] for t in seq] for seq in inputs]\n",
    "targets_idx = [vocab[t]          for t in targets]\n",
    "\n",
    "\n",
    "X = [pad_truncate(seq, context_length, pad_id) for seq in inputs_idx]\n",
    "Y = targets_idx  # cada Y[i] já é um escalar\n",
    "\n",
    "# 3) Tensores e DataLoader\n",
    "X = torch.tensor(X, dtype=torch.long)   # (N, context_length)\n",
    "Y = torch.tensor(Y, dtype=torch.long)   # (N,)\n",
    "ds = TensorDataset(X, Y)\n",
    "loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 4) Otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "# 5) Scheduler com warmup + linear decay\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(\n",
    "        0.0,\n",
    "        float(total_steps - current_step) / float(max(1, total_steps - warmup_steps))\n",
    "    )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adba3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Loop de treinamento\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        # forward\n",
    "        model.tokens_idx([ idx_to_token[id.item()] for id in batch_x[0] ])  # adapte se necessário\n",
    "        logits = model.forward()        # retorna (n_embd,), se seu forward for assim ajuste\n",
    "        # aqui, se seu forward retorna apenas último token, use batchization:\n",
    "        # logits = model(batch_x)  # se tiver implementado batch no forward\n",
    "        # assume logits: (batch_size, vocab_size) ou (vocab_size,) + reshape\n",
    "        # loss\n",
    "        loss = F.cross_entropy(logits.view(-1, model.vocab_size),\n",
    "                               batch_y.view(-1),\n",
    "                               ignore_index=pad_id)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        #if step % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Step {step}/{len(loader)} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    # opcional: validação aqui\n",
    "\n",
    "# ao final, salve checkpoint\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"epoch\": epoch,\n",
    "    \"scheduler\": scheduler.state_dict(),\n",
    "}, \"checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1523b5db",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca9c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da159ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539f18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
