{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f768d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from vocab import tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94982a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_moe_v2.py\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vocab import tokens   # lista global de 209 tokens\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT minimalista – 5 camadas, 4 cabeças, cada camada tem\n",
    "    suas próprias projeções Q, K, V, Wo e seu próprio MLP.\n",
    "\n",
    "    • forward(idx)         → logits (T, vocab_size)\n",
    "    • predict_next_token() → string do próximo token\n",
    "    • predict_all_sentence() → imprime passo-a-passo até max_tokens\n",
    "    • init_tokens(prompt)  → guarda prompt inicial + vocabulário\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, context_length, n_layers, n_heads, n_embd, max_tokens):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hiper-parâmetros\n",
    "        self.vocab_size     = 209\n",
    "        self.context_length = context_length \n",
    "        self.n_layers       = n_layers\n",
    "        self.n_heads        = n_heads\n",
    "        self.n_embd         = n_embd\n",
    "        self.max_tokens     = max_tokens\n",
    "        self.head_dim       = self.n_embd // self.n_heads\n",
    "        assert self.n_embd % self.n_heads == 0, \"n_embd deve ser divisível por n_heads\"\n",
    "\n",
    "        # --- Embeddings -----------------------------------------------------\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embd)\n",
    "        self.wpe = nn.Embedding(self.context_length, self.n_embd)\n",
    "\n",
    "        # --- Projeções Atenção & MLP independentes por camada ---------------\n",
    "        self.qkv_proj = nn.ModuleList([\n",
    "            nn.Linear(self.n_embd, 3 * self.n_embd) for _ in range(self.n_layers)\n",
    "        ])\n",
    "        self.out_proj = nn.ModuleList([\n",
    "            nn.Linear(self.n_embd, self.n_embd) for _ in range(self.n_layers)\n",
    "        ])\n",
    "        self.mlp = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.n_embd, 2 * self.n_embd),\n",
    "                nn.GELU(approximate='tanh'),\n",
    "                nn.Linear(2 * self.n_embd, self.n_embd)\n",
    "            ) for _ in range(self.n_layers)\n",
    "        ])\n",
    "        #self.ln1 = nn.ModuleList([nn.LayerNorm(self.n_embd) for _ in range(self.n_layers)])\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "        self.ln.weight.requires_grad = False   # desliga gradiente de γ\n",
    "        self.ln.bias.requires_grad   = False   # desliga gradiente de β\n",
    "\n",
    "        # --- Máscara causal (bool) ------------------------------------------\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(self.context_length, self.context_length, dtype=torch.bool))\n",
    "        )\n",
    "\n",
    "        # --- Saída final -----------------------------------------------------\n",
    "        #self.final_ln = nn.LayerNorm(self.n_embd)\n",
    "        self.lm_head  = nn.Linear(self.n_embd, self.vocab_size)\n",
    "\n",
    "        # --- Place-holders de prompt / vocabulário --------------------------\n",
    "\n",
    "        self.token_to_idx = {tok: idx for idx, tok in enumerate(tokens)}\n",
    "        self.idx_to_token = {idx: tok for tok, idx in self.token_to_idx.items()}\n",
    "\n",
    "        # congelar parâmetros de todas as LayerNorm\n",
    "        #for ln in list(self.ln1) + list(self.ln2) + [self.final_ln]:\n",
    "        #    ln.weight.requires_grad = False   # desliga gradiente de γ\n",
    "        #    ln.bias.requires_grad   = False   # desliga gradiente de β\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    #  Utilitários de vocabulário / prompt\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def init_tokens(self, tokens_list):\n",
    "        \"\"\"Define prompt inicial e dicionários token↔id.\"\"\"\n",
    "        self.tokens_list  = list(tokens_list)\n",
    "\n",
    "\n",
    "    def _update_indices_tensor(self, seq):\n",
    "        \"\"\"Converte `seq` (lista de tokens) para tensor de índices,\n",
    "           limitando ao último `context_length`.\"\"\"\n",
    "        idxs = [self.token_to_idx[tok] for tok in seq][-self.context_length:]\n",
    "        return torch.tensor(idxs, dtype=torch.long, device=self.wte.weight.device)\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    #  Bloco de atenção (usa pesos da camada `l`)\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def _self_attention(self, x, l):\n",
    "        # x: (T, C)\n",
    "        T, C = x.size()\n",
    "        qkv = self.qkv_proj[l](x)                 # (T, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        q = q.view(T, self.n_heads, self.head_dim).transpose(0, 1)  # (nh,T,hd)\n",
    "        k = k.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        v = v.view(T, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(~self.mask[:T, :T], float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        y   = att @ v                                           # (nh,T,hd)\n",
    "        y   = y.transpose(0, 1).contiguous().view(T, C)         # (T,C)\n",
    "        return self.out_proj[l](y)                              # (T,C)\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    #  Forward: devolve logits para todos os tokens\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        idx : LongTensor (T,) – sequência de índices de tokens.\n",
    "        Retorna logits (T, vocab_size).\n",
    "        \"\"\"\n",
    "        T = idx.size(0)\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        #print(idx)\n",
    "        #print(pos)\n",
    "        x   = self.wte(idx) + self.wpe(pos)\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            x = x + self._self_attention(self.ln(x), l)\n",
    "            x = x + self.mlp[l](self.ln(x))\n",
    "\n",
    "        x = self.ln(x)                      # (T, C)\n",
    "        return self.lm_head(x)                    # (T, vocab_size)\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    #  Geração\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def predict_next_token(self, seq):\n",
    "        \"\"\"\n",
    "        seq : lista de tokens (prompt + já gerados)\n",
    "        Retorna o próximo token (string).\n",
    "        \"\"\"\n",
    "        idx = self._update_indices_tensor(seq)\n",
    "        logits = self.forward(idx)\n",
    "        probs  = torch.softmax(logits[-1], dim=-1)       # último passo\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        return self.idx_to_token[next_id]\n",
    "\n",
    "    def predict_all_sentence(self):\n",
    "        \"\"\"\n",
    "        Gera até `max_tokens` novos tokens SEM alterar o prompt original.\n",
    "        Imprime a frase a cada passo e devolve a lista completa.\n",
    "        \"\"\"\n",
    "        prompt = list(self.tokens_list)          # cópia imutável\n",
    "        generated = []\n",
    "\n",
    "        for step in range(self.max_tokens):\n",
    "            next_tok = self.predict_next_token(prompt + generated)\n",
    "            generated.append(next_tok)\n",
    "            print(f\"Passo {step+1}: {' '.join(prompt + generated)}\")\n",
    "\n",
    "        return prompt + generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1706156a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'eu'),\n",
       " (1, 'você'),\n",
       " (2, 'ele'),\n",
       " (3, 'ela'),\n",
       " (4, 'nós'),\n",
       " (5, 'vocês'),\n",
       " (6, 'eles'),\n",
       " (7, 'elas'),\n",
       " (8, 'me'),\n",
       " (9, 'te'),\n",
       " (10, 'se'),\n",
       " (11, 'nos'),\n",
       " (12, 'vos'),\n",
       " (13, 'lhe'),\n",
       " (14, 'lhes'),\n",
       " (15, 'mim'),\n",
       " (16, 'ti'),\n",
       " (17, 'si'),\n",
       " (18, 'ser'),\n",
       " (19, 'estar'),\n",
       " (20, 'ter'),\n",
       " (21, 'haver'),\n",
       " (22, 'fazer'),\n",
       " (23, 'poder'),\n",
       " (24, 'dizer'),\n",
       " (25, 'ver'),\n",
       " (26, 'dar'),\n",
       " (27, 'saber'),\n",
       " (28, 'querer'),\n",
       " (29, 'chegar'),\n",
       " (30, 'passar'),\n",
       " (31, 'dever'),\n",
       " (32, 'ficar'),\n",
       " (33, 'deixar'),\n",
       " (34, 'pensar'),\n",
       " (35, 'vir'),\n",
       " (36, 'conhecer'),\n",
       " (37, 'casa'),\n",
       " (38, 'tempo'),\n",
       " (39, 'dia'),\n",
       " (40, 'mundo'),\n",
       " (41, 'homem'),\n",
       " (42, 'mulher'),\n",
       " (43, 'vida'),\n",
       " (44, 'mão'),\n",
       " (45, 'olho'),\n",
       " (46, 'palavra'),\n",
       " (47, 'caminho'),\n",
       " (48, 'gato'),\n",
       " (49, 'cachorro'),\n",
       " (50, 'carro'),\n",
       " (51, 'livro'),\n",
       " (52, 'porta'),\n",
       " (53, 'rua'),\n",
       " (54, 'trabalho'),\n",
       " (55, 'dinheiro'),\n",
       " (56, 'noite'),\n",
       " (57, 'bom'),\n",
       " (58, 'mau'),\n",
       " (59, 'feliz'),\n",
       " (60, 'triste'),\n",
       " (61, 'grande'),\n",
       " (62, 'pequeno'),\n",
       " (63, 'novo'),\n",
       " (64, 'velho'),\n",
       " (65, 'forte'),\n",
       " (66, 'fraco'),\n",
       " (67, 'belo'),\n",
       " (68, 'feio'),\n",
       " (69, 'inteligente'),\n",
       " (70, 'rápido'),\n",
       " (71, 'lento'),\n",
       " (72, 'em'),\n",
       " (73, 'de'),\n",
       " (74, 'com'),\n",
       " (75, 'por'),\n",
       " (76, 'para'),\n",
       " (77, 'sobre'),\n",
       " (78, 'entre'),\n",
       " (79, 'até'),\n",
       " (80, 'após'),\n",
       " (81, 'antes'),\n",
       " (82, 'sem'),\n",
       " (83, 'sob'),\n",
       " (84, 'contra'),\n",
       " (85, 'durante'),\n",
       " (86, 'perante'),\n",
       " (87, 'trás'),\n",
       " (88, 'o'),\n",
       " (89, 'a'),\n",
       " (90, 'os'),\n",
       " (91, 'as'),\n",
       " (92, 'um'),\n",
       " (93, 'uma'),\n",
       " (94, 'uns'),\n",
       " (95, 'umas'),\n",
       " (96, 'do'),\n",
       " (97, 'da'),\n",
       " (98, 'dos'),\n",
       " (99, 'das'),\n",
       " (100, 'este'),\n",
       " (101, 'esta'),\n",
       " (102, 'estes'),\n",
       " (103, 'estas'),\n",
       " (104, 'um'),\n",
       " (105, 'dois'),\n",
       " (106, 'três'),\n",
       " (107, 'quatro'),\n",
       " (108, 'cinco'),\n",
       " (109, 'seis'),\n",
       " (110, 'sete'),\n",
       " (111, 'oito'),\n",
       " (112, 'nove'),\n",
       " (113, 'dez'),\n",
       " (114, 'vinte'),\n",
       " (115, 'trinta'),\n",
       " (116, 'cem'),\n",
       " (117, 'mil'),\n",
       " (118, 'milhão'),\n",
       " (119, 'sim'),\n",
       " (120, 'não'),\n",
       " (121, 'talvez'),\n",
       " (122, 'hoje'),\n",
       " (123, 'amanhã'),\n",
       " (124, 'ontem'),\n",
       " (125, 'agora'),\n",
       " (126, 'sempre'),\n",
       " (127, 'nunca'),\n",
       " (128, 'já'),\n",
       " (129, 'ainda'),\n",
       " (130, 'depois'),\n",
       " (131, 'antes'),\n",
       " (132, 'aqui'),\n",
       " (133, 'ali'),\n",
       " (134, 'lá'),\n",
       " (135, 'longe'),\n",
       " (136, 'perto'),\n",
       " (137, 'escola'),\n",
       " (138, 'cidade'),\n",
       " (139, 'país'),\n",
       " (140, 'terra'),\n",
       " (141, 'céu'),\n",
       " (142, 'mar'),\n",
       " (143, 'praia'),\n",
       " (144, 'montanha'),\n",
       " (145, 'rio'),\n",
       " (146, 'floresta'),\n",
       " (147, 'livraria'),\n",
       " (148, 'mercado'),\n",
       " (149, 'amigo'),\n",
       " (150, 'amiga'),\n",
       " (151, 'criança'),\n",
       " (152, 'professor'),\n",
       " (153, 'aluno'),\n",
       " (154, 'trabalho'),\n",
       " (155, 'pessoa'),\n",
       " (156, 'família'),\n",
       " (157, 'história'),\n",
       " (158, 'amor'),\n",
       " (159, 'vou'),\n",
       " (160, 'vai'),\n",
       " (161, 'vamos'),\n",
       " (162, 'vão'),\n",
       " (163, 'fui'),\n",
       " (164, 'foi'),\n",
       " (165, 'foram'),\n",
       " (166, 'estou'),\n",
       " (167, 'está'),\n",
       " (168, 'estamos'),\n",
       " (169, 'estão'),\n",
       " (170, 'gostar'),\n",
       " (171, 'amar'),\n",
       " (172, 'odiar'),\n",
       " (173, 'correr'),\n",
       " (174, 'andar'),\n",
       " (175, 'viajar'),\n",
       " (176, 'sorrir'),\n",
       " (177, 'chorar'),\n",
       " (178, 'brincar'),\n",
       " (179, 'estudar'),\n",
       " (180, 'beber'),\n",
       " (181, 'comer'),\n",
       " (182, 'dormir'),\n",
       " (183, 'acordar'),\n",
       " (184, 'abrir'),\n",
       " (185, 'fechar'),\n",
       " (186, 'comprar'),\n",
       " (187, 'vender'),\n",
       " (188, 'ligar'),\n",
       " (189, 'desligar'),\n",
       " (190, 'e'),\n",
       " (191, 'mas'),\n",
       " (192, 'ou'),\n",
       " (193, 'porque'),\n",
       " (194, 'que'),\n",
       " (195, 'como'),\n",
       " (196, 'quando'),\n",
       " (197, 'onde'),\n",
       " (198, 'quem'),\n",
       " (199, 'qual'),\n",
       " (200, 'podem'),\n",
       " (201, 'explorar'),\n",
       " (202, 'universidade'),\n",
       " (203, 'experiência'),\n",
       " (204, 'interessante'),\n",
       " (205, 'no'),\n",
       " (206, 'na'),\n",
       " (207, 'felizes'),\n",
       " (208, 'tivemos')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4204617",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 20\n",
    "n_layers       = 3\n",
    "n_heads        = 2\n",
    "n_embd         = 64\n",
    "max_tokens     = 12\n",
    "tokens_list = [\"o\", \"gato\", \"pequeno\"]\n",
    "model = GPT(context_length, n_layers, n_heads, n_embd, max_tokens)\n",
    "model.init_tokens(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fc8018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1: o gato pequeno perto\n",
      "Passo 2: o gato pequeno perto história\n",
      "Passo 3: o gato pequeno perto história livro\n",
      "Passo 4: o gato pequeno perto história livro quatro\n",
      "Passo 5: o gato pequeno perto história livro quatro este\n",
      "Passo 6: o gato pequeno perto história livro quatro este com\n",
      "Passo 7: o gato pequeno perto história livro quatro este com conhecer\n",
      "Passo 8: o gato pequeno perto história livro quatro este com conhecer país\n",
      "Passo 9: o gato pequeno perto história livro quatro este com conhecer país ontem\n",
      "Passo 10: o gato pequeno perto história livro quatro este com conhecer país ontem vida\n",
      "Passo 11: o gato pequeno perto história livro quatro este com conhecer país ontem vida está\n",
      "Passo 12: o gato pequeno perto história livro quatro este com conhecer país ontem vida está foi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'gato',\n",
       " 'pequeno',\n",
       " 'perto',\n",
       " 'história',\n",
       " 'livro',\n",
       " 'quatro',\n",
       " 'este',\n",
       " 'com',\n",
       " 'conhecer',\n",
       " 'país',\n",
       " 'ontem',\n",
       " 'vida',\n",
       " 'está',\n",
       " 'foi']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.idx_to_token[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Nome: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Valores: {param}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ba1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344390bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total treinável: {total:,d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49746aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:<40} {param.numel():>10,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f9bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d72df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_all_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50974d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
